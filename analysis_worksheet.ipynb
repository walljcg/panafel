{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that need to use pythonnet 2.3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,os,gc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arcpy\n",
    "import ee\n",
    "from scipy import stats\n",
    "#import rpy2\n",
    "\n",
    "#Set arcpy to overwrite output\n",
    "arcpy.env.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clr, System\n",
    "sys.path.append(r\".\\code_libs\\arcmet_code_libs_10.7.1\")\n",
    "clr.AddReference(\"System.Collections\")\n",
    "clr.AddReference(\"ArcMET_Base\") #Import ArcMET_Base.dll (Movement Ecology Tools for ArcGIS - www.movementecology.net)\n",
    "from ArcMET_Base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Import AnimalTracking2_Esri_DB.dll\n",
    "# clr.AddReference(\"AnimalTracking2_Esri_DB\")\n",
    "# from AnimalTracking2_Esri_DB import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ArcGIS_EE_Utilities\n",
    "from ArcGIS_EE_Utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDrive='C:\\\\Users\\\\jake_\\\\dropbox'\n",
    "rootFolder = rootDrive + '\\\\Research_PanAfEl\\\\'\n",
    "MovDataFolder = rootFolder + 'Movement Data'\n",
    "RangeFolder =  rootFolder + 'Analyses\\\\Ranges'\n",
    "etdFolder =  RangeFolder + '\\\\ETD'\n",
    "mcpFolder =  RangeFolder + '\\\\MCP'\n",
    "kdeFolder =  RangeFolder + '\\\\KDE'\n",
    "tempmetricsFolder = rootFolder + 'Analyses\\\\TemporalMetrics'\n",
    "trajPathMetricsFolder = rootFolder + 'Analyses\\\\TrajPathMetrics'\n",
    "SpatialDataFolder = rootFolder + 'Spatial Data'\n",
    "figuresFolder = rootFolder + 'Figures'\n",
    "tablesFolder = rootFolder + 'Tables'\n",
    "\n",
    "etd16DayAnalysisTable = tablesFolder + '\\\\ETD_16Day_Analysis_Table.csv'\n",
    "etdAnnualAnalysisTable = tablesFolder + '\\\\ETD_Annual_Analysis_Table.csv'\n",
    "etd16DayAnalysisTableFilt = tablesFolder + '\\\\ETD_16Day_Analysis_Table_Filtered.csv'\n",
    "etdAnnualAnalysisTableFilt = tablesFolder + '\\\\ETD_Annual_Analysis_Table_Filtered.csv'\n",
    "AnnualAnalysisTable = tablesFolder + '\\\\Annual_Analysis_Table.csv'\n",
    "\n",
    "#Define a file with subject & dates that signal compromised data not to be included in analysis\n",
    "ignoreDataFile = rootFolder + 'Analyses\\\\ignoreData.csv'\n",
    "\n",
    "#AnimalTracking DB\n",
    "animalTrackingDB_Path= MovDataFolder + \"\\\\AnimalTracking.gdb\"\n",
    "tmTable = animalTrackingDB_Path + \"\\\\TRACKINGMASTER\"\n",
    "\n",
    "#GCS Spatial Reference\n",
    "wgs84GCS =\"GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]]\" + \\\n",
    "\",PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modisStartDate = System.DateTime(2000,3,5,0,0,0) #This is the earliest valid MODIS imagery date\n",
    "startDate = System.DateTime(1998,2,6,8,0,0) #The first datapoint in the database\n",
    "endDate = System.DateTime(2014,1,1,0,0,0) #Run the analysis up to the end of 2013\n",
    "\n",
    "#Define the desired range percentiles\n",
    "percentiles = System.Collections.Generic.List[System.Double]()\n",
    "percentiles.Add(System.Double(0.5))\n",
    "percentiles.Add(System.Double(0.90))\n",
    "percentiles.Add(System.Double(0.95))\n",
    "percentiles.Add(System.Double(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the output database properties\n",
    "MovGDB_Path = MovDataFolder + \"\\\\MovData.gdb\"\n",
    "MovGDB_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "MovGDB_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "MovGDB_PropVals.Add(System.Tuple[str,str](\"DATABASE\", MovGDB_Path))\n",
    "MovGDB_Props = GeodatabaseHelper.CreateConnection(MovGDB_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the output databases/props for the 16-Day & Annual ETD Ranges\n",
    "etdRanges16Day_Path = etdFolder + \"\\\\ETD16DayRanges.gdb\"\n",
    "etdRanges16Day_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "etdRanges16Day_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "etdRanges16Day_PropVals.Add(System.Tuple[str,str](\"DATABASE\", etdRanges16Day_Path))\n",
    "etdRanges16Day_Props = GeodatabaseHelper.CreateConnection(etdRanges16Day_PropVals)\n",
    "\n",
    "etdRangesAnnual_Path = etdFolder + \"\\\\ETDAnnualRanges.gdb\"\n",
    "etdRangesAnnual_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "etdRangesAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "etdRangesAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASE\", etdRangesAnnual_Path))\n",
    "etdRangesAnnual_Props = GeodatabaseHelper.CreateConnection(etdRangesAnnual_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the output databases/props for the 16-Day & Annual KDE Ranges\n",
    "kdeRanges16Day_Path = kdeFolder + \"\\\\KDE16DayRanges.gdb\"\n",
    "kdeRanges16Day_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "kdeRanges16Day_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "kdeRanges16Day_PropVals.Add(System.Tuple[str,str](\"DATABASE\", kdeRanges16Day_Path))\n",
    "kdeRanges16Day_Props = GeodatabaseHelper.CreateConnection(kdeRanges16Day_PropVals)\n",
    "\n",
    "kdeRangesAnnual_Path = kdeFolder + \"\\\\KDEAnnualRanges.gdb\"\n",
    "kdeRangesAnnual_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "kdeRangesAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "kdeRangesAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASE\", kdeRangesAnnual_Path))\n",
    "kdeRangesAnnual_Props = GeodatabaseHelper.CreateConnection(kdeRangesAnnual_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the output databases/props for the 16-Day & Annual MCP Ranges\n",
    "mcpRanges16Day_Path = mcpFolder + \"\\\\MCP16DayRanges.gdb\"\n",
    "mcpRanges16Day_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "mcpRanges16Day_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "mcpRanges16Day_PropVals.Add(System.Tuple[str,str](\"DATABASE\", mcpRanges16Day_Path))\n",
    "mcpRanges16Day_Props = GeodatabaseHelper.CreateConnection(mcpRanges16Day_PropVals)\n",
    "\n",
    "mcpRangesAnnual_Path = mcpFolder + \"\\\\MCPAnnualRanges.gdb\"\n",
    "mcpRangesAnnual_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "mcpRangesAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "mcpRangesAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASE\", mcpRangesAnnual_Path))\n",
    "mcpRangesAnnual_Props = GeodatabaseHelper.CreateConnection(mcpRangesAnnual_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Temporality databases\n",
    "tempmetrics16Day_Path = tempmetricsFolder + \"\\\\TempMetrics16Day.gdb\"\n",
    "tempmetrics16Day_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "tempmetrics16Day_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "tempmetrics16Day_PropVals.Add(System.Tuple[str,str](\"DATABASE\", tempmetrics16Day_Path))\n",
    "tempmetrics16Day_Props = GeodatabaseHelper.CreateConnection(tempmetrics16Day_PropVals)\n",
    "\n",
    "tempmetricsAnnual_Path = tempmetricsFolder + \"\\\\TempMetricsAnnual.gdb\"\n",
    "tempmetricsAnnual_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "tempmetricsAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "tempmetricsAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASE\", tempmetricsAnnual_Path))\n",
    "tempmetricsAnnual_Props = GeodatabaseHelper.CreateConnection(tempmetricsAnnual_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TrajPathMetrics database\n",
    "trajPathMetrics16Day_Path = trajPathMetricsFolder + \"\\\\TrajPathMetrics16Day.gdb\"\n",
    "trajPathMetrics16Day_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "trajPathMetrics16Day_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "trajPathMetrics16Day_PropVals.Add(System.Tuple[str,str](\"DATABASE\", trajPathMetrics16Day_Path))\n",
    "trajPathMetrics16Day_Props = GeodatabaseHelper.CreateConnection(trajPathMetrics16Day_PropVals)\n",
    "\n",
    "trajPathMetricsAnnual_Path = trajPathMetricsFolder + \"\\\\TrajPathMetricsAnnual.gdb\"\n",
    "trajPathMetricsAnnual_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "trajPathMetricsAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "trajPathMetricsAnnual_PropVals.Add(System.Tuple[str,str](\"DATABASE\", trajPathMetricsAnnual_Path))\n",
    "trajPathMetricsAnnual_Props = GeodatabaseHelper.CreateConnection(trajPathMetricsAnnual_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define database for the WDPA feature data\n",
    "wdpaGDB_Path = SpatialDataFolder + \"\\\\WDPA_August2013.gdb\"\n",
    "wdpaGDB_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "wdpaGDB_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "wdpaGDB_PropVals.Add(System.Tuple[str,str](\"DATABASE\", wdpaGDB_Path))\n",
    "wdpaGDB_Props = GeodatabaseHelper.CreateConnection(wdpaGDB_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Regions database\n",
    "regionsGDB_Path = SpatialDataFolder + \"\\\\Regions.gdb\"\n",
    "regionsGDB_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "regionsGDB_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "regionsGDB_PropVals.Add(System.Tuple[str,str](\"DATABASE\", regionsGDB_Path))\n",
    "regionsGDB_Props = GeodatabaseHelper.CreateConnection(regionsGDB_PropVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Spatial database\n",
    "spatialGDB_Path = SpatialDataFolder + \"\\\\SpatialData.gdb\"\n",
    "spatialGDB_PropVals = System.Collections.Generic.List[System.Tuple[str,str]]()\n",
    "spatialGDB_PropVals.Add(System.Tuple[str,str](\"DATABASETYPE\", \"FGDB\"))\n",
    "spatialGDB_PropVals.Add(System.Tuple[str,str](\"DATABASE\", spatialGDB_Path))\n",
    "spatialGDB_Props = GeodatabaseHelper.CreateConnection(spatialGDB_PropVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnimalTracking Database, Regions & TrackingMaster tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBI = TrackingDatabaseESRIAdapter(\"DB_CONNECTION_PROPERTIES=ATDB_Connection,DATABASETYPE=FGDB,DATABASE=\" + animalTrackingDB_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join TM & Regions tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Manually export the regions table to a csv from the server Postgres database\n",
    "#   COPY Regions TO 'E:/Research/2015-04-01 PanAfEl Manuscript/Spatial Data/Regions.csv' CSV HEADER;\n",
    "#2. Manually create a Regions.gdb database in the 'Spatial Data' folder\n",
    "#3. Import regions csv file to a table called 'Regions' to the Regions.gdb database:\n",
    "\n",
    "arcpy.TableToTable_conversion(in_rows=SpatialDataFolder + \"\\\\Regions.csv\",\n",
    "                              out_path=SpatialGDB_Path + \"\\\\Regions.gdb\", out_name=\"Regions\")\n",
    "\n",
    "#4. Manually create a 'MetaRegions' feature class in the Regions.gdb database with polygons for each meta-region:\n",
    "#   Central,South,East,West\n",
    "#5. Add a 'MetaRegion' column to the Regions feature class and populate with appropriate metaregion designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the TrackingMaster table with the Regions table (only need to do this once)\n",
    "#arcpy.JoinField_management(animalTrackingDB_Path + \"\\\\TRACKINGMASTER\", \"Chronofile\", \n",
    "#                           regionsGDB_Path + \"\\\\Regions\",\"Chronofile\",[\"Region\",\"Country\",\"MetaRegion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movement Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from AnimalTracking Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select both species from the db\n",
    "elesQuery = \"SPECIES='Elephant' OR SPECIES='Forest Elephant'\"\n",
    "\n",
    "#Open the FeatureClass that will define the Metaregions\n",
    "regionFC = FeatureDataHelper.ReturnFeatureClass2(regionsGDB_Props,\"MetaRegionsDissolve\")\n",
    "animalInfoList = DBI.GetAnimalInfoList(elesQuery,regionFC) #Use a regional based spatial query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a TrajectoryFilter\n",
    "minTime = System.TimeSpan.FromMinutes(2.0) #The minimum time needed between points otherwise the later point gets filtered\n",
    "trajFilt = TrajectoryFilter(0.5,minTime,8.0,startDate,endDate) #MinDist_Meters, MinTime_Minutes, MaxSpeed_Km/Hr, Start, End\n",
    "\n",
    "#Define the DatasetExtractionParameters object\n",
    "DEP = DatasetExtractionParameters()\n",
    "DEP.AnimalTrackingDB = DBI\n",
    "DEP.MergeType = MergeOption.ByAnimal #Because data needs to be in UTM then had to do separate feature classes and not merge them\n",
    "DEP.startTimeOption = StartTimeOption.MostRecent\n",
    "DEP.endTimeOption = EndTimeOption.LessRecent\n",
    "DEP.startTime = startDate\n",
    "DEP.endTime = endDate\n",
    "DEP.PointFilter = trajFilt #filter the data\n",
    "\n",
    "#Define the DatasetStorageParameters object\n",
    "DSP = ESRIDatasetStorage()\n",
    "DSP.LyrFilePath = MovDataFolder\n",
    "DSP.OutputUTM = True #Will calculating areas so need projected coordinates\n",
    "DSP.CreateTracks = False \n",
    "DSP.OutputDBProps = MovGDB_Props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the extraction\n",
    "MDE = MovementDataExtractor()\n",
    "MDE.ExtractDatasets(animalInfoList, DEP, DSP);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete entire feature classes that shouldn't be included in analysis\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\Wangari_Maathai_Locs\") #Data reflected at the equator and so can't tell what's N vs S\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\FortyFive_Locs\") #Single animal from Botswana\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\Kiramatian_Locs\")  #ACC elephant, not STE\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\Lorna_Locs\")  #ACC elephant, not STE\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\Dionysus_Locs\")  #Zim elephant, not STE\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\Thor_Locs\")  #Zim elephant, not STE\n",
    "arcpy.Delete_management (MovGDB_Path + \"\\\\Zeus_Locs\")  #Zim elephant, not STE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to delete individual datapoints\n",
    "def delFeatures(cleanName, sqlexpression):\n",
    "    tempLayer = \"movSelect\"\n",
    "    arcpy.MakeFeatureLayer_management(MovGDB_Path + \"\\\\\" + cleanName, tempLayer)\n",
    "    arcpy.SelectLayerByAttribute_management(tempLayer, \"NEW_SELECTION\", sqlexpression)\n",
    "    if int(arcpy.GetCount_management(tempLayer).getOutput(0)) > 0:\n",
    "        delCount = arcpy.DeleteFeatures_management(tempLayer)\n",
    "    arcpy.Delete_management( tempLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This animal was physically confined within a fence for a period of time at start of tracking dataset\n",
    "#Delete the records before elephant was free ranging\n",
    "name = 'Ngelesha'\n",
    "sqlexpression = \"MovDataID='\" + name + \"' AND fixtime < date'2008-11-08 10:00:26'\"\n",
    "cleanName = FeatureDataHelper.CleanupName(name) + '_locs'\n",
    "delFeatures(cleanName,sqlexpression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual Point deletions\n",
    "#Manually identify points for deletion from each animal's dataset that weren't picked up by the filter\n",
    "#Requires examining each dataset for obvious errors and listing them in csv file\n",
    "pointsToDel = pd.read_csv(MovDataFolder + \"\\\\Filter_Indiv_Points.csv\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pointsToDel.index)):\n",
    "    name = pointsToDel['Name'][i]\n",
    "    sqlexpression = \"MovDataID='\" + name + \"' AND fixtime = date'\" + pointsToDel['Fixtime'][i] + \"'\"\n",
    "    cleanName = FeatureDataHelper.CleanupName(name) + '_locs'\n",
    "    try:\n",
    "        delFeatures(cleanName,sqlexpression)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ele Names Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(table, field):\n",
    "    with arcpy.da.SearchCursor(table, [field]) as cursor:\n",
    "        return sorted({row[0] for row in cursor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace=MovGDB_Path\n",
    "fcs = arcpy.ListFeatureClasses(\"*_Locs\")\n",
    "fcCount=0\n",
    "countFeatures=0\n",
    "eleNamesArray=[]\n",
    "for fc in fcs:\n",
    "    thisCount = int(arcpy.GetCount_management(fc).getOutput(0))\n",
    "    countFeatures += thisCount\n",
    "    if thisCount > 0:\n",
    "        fcCount+=1\n",
    "        eleNamesArray.append(str(unique_values(fc,'MovDataID')[0])) #Grab the first unique value (should just be one)\n",
    "    \n",
    "del(fcs)\n",
    "print fcCount, countFeatures\n",
    "eleNamesArray = sorted(eleNamesArray)\n",
    "print eleNamesArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project & Merge Movement Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be useful, in some cases, to have a single feature class with all movement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace=MovGDB_Path\n",
    "arcpy.env.Overwrite=True\n",
    "fcs = arcpy.ListFeatureClasses(\"*_Locs\")\n",
    "for fc in fcs:\n",
    "    arcpy.Project_management(in_dataset=fc, out_dataset=fc + '_gcs', out_coor_system=wgs84GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all projected movement feature classes\n",
    "arcpy.env.workspace=MovGDB_Path\n",
    "arcpy.env.Overwrite=True\n",
    "fcs = arcpy.ListFeatureClasses(\"*_gcs\")\n",
    "mergeGCSDatasets=[]\n",
    "for fc in fcs:\n",
    "    mergeGCSDatasets.append(fc)\n",
    "    \n",
    "arcpy.Merge_management(mergeGCSDatasets, MovGDB_Path + \"\\\\MergeAllLocsGCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Window Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = MovingWindow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp = MCPRange()\n",
    "def CalcMCPRange(traj,calcParams):\n",
    "    try:\n",
    "        theresult = mcp.CalculateMCPRange(traj, calcParams)\n",
    "        theresult.WriteCalcSummaryTable('MCPSummaryTable', calcParams.dbProps)\n",
    "        del(theresult)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "#Define the function delegate\n",
    "mcpRangeFuncDelegate = MovingWindow.MethodToIterate(CalcMCPRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etd = EllipticalTimeDensityRange()\n",
    "def CalcETDRange(traj, calcParams): \n",
    "    #Define the ETD function that will run for each moving window timespan\n",
    "    try:\n",
    "        result = etd.CalculateETDRange(traj, calcParams)\n",
    "        result.WriteCalcSummaryTable('ETDSummaryTable', calcParams.dbProps)\n",
    "        del result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "#Define the function delegate\n",
    "etdRangeFuncDelegate = MovingWindow.MethodToIterate(CalcETDRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensityRange()\n",
    "def CalcKDERange(traj, calcParams): \n",
    "    #Define the ETD function that will run for each moving window timespan\n",
    "    try:\n",
    "        result = kde.CalculateKernelDensityRange(traj, calcParams)\n",
    "        result.WriteCalcSummaryTable('KDESummaryTable', calcParams.dbProps)\n",
    "        del result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "#Define the function delegate\n",
    "kdeRangeFuncDelegate = MovingWindow.MethodToIterate(CalcKDERange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempMet = TemporalMetrics()\n",
    "def CalcTempMetrics(traj,calcParams):\n",
    "    try:\n",
    "        tempMet.CalculateTemporalityMetrics(traj, calcParams)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "tempMetFuncDelegate = MovingWindow.MethodToIterate(CalcTempMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Trajectory Path Metrics\n",
    "trajPathMetrics = TrajectoryPathMetrics()\n",
    "def trajPathMetricsFunc(traj,calcParams):\n",
    "    theresult = trajPathMetrics.CalculateTrajectoryPathMetrics(traj, calcParams)\n",
    "    #theresult.WriteCalcSummaryTable('TrajPathMetricsCalcResultsTable', TrajPathMetricsGDB_Props)\n",
    "    del(theresult)\n",
    "trajPathMetricsDelegate = MovingWindow.MethodToIterate(trajPathMetricsFunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MCP Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate 16-Day & yearly MCP Ranges for all elephants\n",
    "t1=dt.datetime.now()\n",
    "\n",
    "for name in eleNamesArray:\n",
    "    print(\"Analysing: \" + name)\n",
    "    cleanName = FeatureDataHelper.CleanupName(name) + '_Locs'\n",
    "    \n",
    "    #Open the movement data feature class\n",
    "    MovFC = FeatureDataHelper.ReturnFeatureClass2(MovGDB_Props,cleanName)\n",
    "\n",
    "    #Create a trajectory\n",
    "    path = TrajectoryEngine.CreateTrajectoryFromPointFeatureClass(MovFC,'',\"Fixtime\",True)\n",
    "\n",
    "    #Define the CalcID\n",
    "    calcID = cleanName \n",
    "\n",
    "    #Calculate 16-Day Windows\n",
    "    modisDateObject = MODIS16DayWindowUnit(path.StartTime)\n",
    "    windowDates16Day = modisDateObject.DefineWindowTimes(path.StartTime,path.EndTime)\n",
    "    \n",
    "    #Calculate the MCP 16-Day Ranges\n",
    "    mcpParams16Day = MCPRangeCalcParameters(percentiles,cleanName,True,name,calcID,\n",
    "                                            path.StartTime,path.EndTime,mcpRanges16Day_Props)\n",
    "    #Run the 16-day calculation\n",
    "    window.IterateWindow(path,windowDates16Day,False,True,mcpRangeFuncDelegate,mcpParams16Day)\n",
    "    \n",
    "    #Calculate Annual Windows\n",
    "    annualWindowObject = YearWindowUnit(path.StartTime,1)\n",
    "    windowDatesAnnual = annualWindowObject.DefineWindowTimes(path.StartTime,path.EndTime,1.0)\n",
    "    \n",
    "    #Calculate the MCP Annual Range\n",
    "    mcpParamsAnnual = MCPRangeCalcParameters(percentiles,cleanName,True,name,calcID,\n",
    "                                             path.StartTime,path.EndTime,mcpRangesAnnual_Props)\n",
    "    \n",
    "    #Run the Annual calculation\n",
    "    window.IterateWindow(path, windowDatesAnnual,False,True,mcpRangeFuncDelegate,mcpParamsAnnual)\n",
    "      \n",
    "    #Cleanup\n",
    "    del path,MovFC,modisDateObject, windowDates16Day,mcpParams16Day,annualWindowObject,windowDatesAnnual,mcpParamsAnnual\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "t2=dt.datetime.now()\n",
    "deltaT=t2-t1\n",
    "print('The MCP calculation took: ' + str(deltaT.total_seconds()/60) + ' minutes.') #22.12 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project & Merge MCP Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project all 16-Day mcp ranges to common GCS projection\n",
    "arcpy.env.workspace=mcpRanges16Day_Path\n",
    "arcpy.env.Overwrite=True\n",
    "fcs = arcpy.ListFeatureClasses()\n",
    "for fc in fcs:\n",
    "    arcpy.Project_management(in_dataset=fc, out_dataset=fc + '_gcs', out_coor_system=wgs84GCS)\n",
    "del(fcs)\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all 16-day GCS mcp ranges\n",
    "arcpy.env.workspace=mcpRanges16Day_Path\n",
    "fcs = arcpy.ListFeatureClasses(\"*_gcs\")\n",
    "mergeGCSDatasets=[]\n",
    "for fc in fcs:\n",
    "    mergeGCSDatasets.append(fc)  \n",
    "arcpy.Merge_management(mergeGCSDatasets, mcpRanges16Day_Path + \"\\\\MCP16DayPolygonsGCS\")\n",
    "del(fcs)\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project all annual mcp ranges to common GCS projection\n",
    "arcpy.env.workspace=mcpRangesAnnual_Path\n",
    "arcpy.env.Overwrite=True\n",
    "fcs = arcpy.ListFeatureClasses()\n",
    "for fc in fcs:\n",
    "    arcpy.Project_management(in_dataset=fc, out_dataset=fc + '_gcs', out_coor_system=wgs84GCS)\n",
    "del(fcs)\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all annual GCS mcp ranges\n",
    "arcpy.env.workspace=mcpRangesAnnual_Path\n",
    "arcpy.env.Overwrite=True\n",
    "fcs = arcpy.ListFeatureClasses(\"*_gcs\")\n",
    "mergeGCSDatasets=[]\n",
    "for fc in fcs:\n",
    "    mergeGCSDatasets.append(fc)\n",
    "arcpy.Merge_management(mergeGCSDatasets, mcpRangesAnnual_Path + \"\\\\MCPAnnualPolygonsGCS\")\n",
    "del(fcs)\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dissolve the merged yearly ranges\n",
    "arcpy.env.workspace=mcpRangesAnnual_Path\n",
    "arcpy.env.Overwrite=True\n",
    "arcpy.Dissolve_management(\"MCPAnnualPolygonsGCS\", \"MCPAnnualPolygonsGCSDissolv\")\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project the dissolved MCP polygons to Africa Albers Equal Area Conic projection to get the area in Km2\n",
    "arcpy.env.workspace=mcpRangesAnnual_Path\n",
    "arcpy.env.Overwrite=True\n",
    "arcpy.Project_management(in_dataset=\"MCPAnnualPolygonsGCSDissolv\", \n",
    "                         out_dataset=\"MCPAnnualPolygonsAlbersDissolv\", \n",
    "                         out_coor_system=arcpy.SpatialReference(102022))\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDE Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this function if you need to re-run the KDE/ETD calculation for some reason \n",
    "#(e.g., the first time through there was an error with some grids\n",
    "#and you want to run it again). This function will check the results table and \n",
    "#if a calc with the given start/end dates exists then\n",
    "#it will remove those dates from the input windowDates array\n",
    "\n",
    "# first delete any row entries that have an error\n",
    "def alreadyRun(MovDataID, resultsDBPath, resultsTableName, windowDates):\n",
    "    resultsTable = resultsDBPath + '\\\\' + resultsTableName \n",
    "    whereClause = \"MovDataID='\" + MovDataID + \"'\"\n",
    "    cursor = arcpy.SearchCursor(dataset=resultsTable, where_clause=whereClause)\n",
    "    for row in cursor:\n",
    "        \n",
    "        #remove the current dates from the windowDates list for any previously successful run\n",
    "        curStart = row.getValue(\"StartDate\")\n",
    "        curEnd = row.getValue(\"EndDate\")\n",
    "        \n",
    "        #Create a .Net Tuple\n",
    "        curWin = System.Tuple[System.DateTime,System.DateTime](System.DateTime.Parse(curStart.strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
    "                                                               System.DateTime.Parse(curEnd.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "        \n",
    "        #print 'Removing' + curWin.ToString()\n",
    "        windowDates.Remove(curWin)\n",
    "    del cursor\n",
    "    return windowDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate KDE Ranges for all elephants\n",
    "t1=dt.datetime.now()\n",
    "\n",
    "for name in eleNamesArray:\n",
    "    print(\"Analysing: \" + name)\n",
    "    cleanName = FeatureDataHelper.CleanupName(name) + '_Locs'\n",
    "    \n",
    "    #Open the movement data feature class\n",
    "    MovFC = FeatureDataHelper.ReturnFeatureClass2(MovGDB_Props,cleanName)\n",
    "\n",
    "    #Create a trajectory\n",
    "    path = TrajectoryEngine.CreateTrajectoryFromPointFeatureClass(MovFC,'',\"Fixtime\",True)\n",
    "\n",
    "    #Define the CalcID\n",
    "    calcID = cleanName\n",
    "\n",
    "    #Calculate 16-Day Windows\n",
    "    modisDateObject = MODIS16DayWindowUnit(path.StartTime)\n",
    "    windowDates16Day = modisDateObject.DefineWindowTimes(path.StartTime,path.EndTime)\n",
    "\n",
    "    #Calculate Annual Windows\n",
    "    annualWindowObject = YearWindowUnit(path.StartTime,1)\n",
    "    windowDatesAnnual = annualWindowObject.DefineWindowTimes(path.StartTime,path.EndTime,1.0)\n",
    "\n",
    "    #Skip dates where the calc was already run\n",
    "    windowDates16Day = alreadyRun(name, kdeRanges16Day_Path,'KDESummaryTable', windowDates16Day)\n",
    "    windowDatesAnnual = alreadyRun(name, kdeRangesAnnual_Path,'KDESummaryTable', windowDatesAnnual)\n",
    "\n",
    "    #Calculate the KDE 16-Day Range \n",
    "    if windowDates16Day.Count > 0:\n",
    "        \n",
    "        kdeParams16Day = KernelDensityRangeCalcParameters( 0., # smoothingParameter\n",
    "                                                          'h_Ref', # smoothingMethod\n",
    "                                                           6, # MaxSD\n",
    "                                                           1E-20, # ProbCutOffVal\n",
    "                                                           cleanName + '_etd', # outRasterName\n",
    "                                                           1.3, # rasterExpansionRatio\n",
    "                                                           200., # rasterResolution\n",
    "                                                           name, # movDataID\n",
    "                                                           calcID, # calcID\n",
    "                                                           path.StartTime, # start\n",
    "                                                           path.EndTime, # end\n",
    "                                                           kdeRanges16Day_Props) # dbProps\n",
    "        \n",
    "        #Run the calculation\n",
    "        window.IterateWindow(path, \n",
    "                             windowDates16Day,\n",
    "                             False,\n",
    "                             True,\n",
    "                             kdeRangeFuncDelegate,\n",
    "                             kdeParams16Day)\n",
    "        \n",
    "        #Cleanup\n",
    "        del kdeParams16Day\n",
    "\n",
    "    #Calculate the KDE Annual Range\n",
    "    if windowDatesAnnual.Count > 0:\n",
    "        \n",
    "        kdeParamsAnnual = KernelDensityRangeCalcParameters(0.,\n",
    "                                                          'h_Ref',\n",
    "                                                           6,\n",
    "                                                           1E-20,\n",
    "                                                           cleanName + '_etd',\n",
    "                                                           1.3,\n",
    "                                                           200.,\n",
    "                                                           name,\n",
    "                                                           calcID,\n",
    "                                                           path.StartTime,\n",
    "                                                           path.EndTime,\n",
    "                                                           kdeRangesAnnual_Props)\n",
    "         \n",
    "        \n",
    "        #Run the calculation\n",
    "        window.IterateWindow(path,\n",
    "                             windowDatesAnnual,\n",
    "                             False,\n",
    "                             True,\n",
    "                             kdeRangeFuncDelegate,\n",
    "                             kdeParamsAnnual)\n",
    "        \n",
    "        #Cleanup\n",
    "        del kdeParamsAnnual\n",
    "\n",
    "    #Cleanup\n",
    "    del path, modisDateObject, annualWindowObject, windowDates16Day, windowDatesAnnual\n",
    "\n",
    "t2=dt.datetime.now()\n",
    "deltaT=t2-t1\n",
    "print('The KDE calculation took: ' + str(deltaT.total_seconds()/60) + ' minutes.') #2810.91 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up 16-Day KDE database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out the names of all non-null rasters from the ETD Summary Table\n",
    "resultsTable = kdeRanges16Day_Path + '\\\\' + 'KDESummaryTable'\n",
    "nonNullKDERasters = []\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        nonNullKDERasters.append(ResultRaster)\n",
    "del cursor\n",
    "print str(len(nonNullKDERasters))\n",
    "\n",
    "#Determine any rasters that are not part of the non-null list\n",
    "arcpy.env.workspace=kdeRanges16Day_Path\n",
    "rastToDel = []\n",
    "rastCursor = arcpy.ListRasters()\n",
    "for rast in rastCursor:\n",
    "    if str(rast) not in nonNullKDERasters:\n",
    "        rastToDel.append(str(rast))\n",
    "del rastCursor\n",
    "print str(len(rastToDel))\n",
    "\n",
    "#delete these orphaned rasters from the database to clean it up\n",
    "count=0\n",
    "for delRast in rastToDel:\n",
    "    count=count+1\n",
    "    print str(count)\n",
    "    arcpy.Delete_management(delRast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Annual KDE database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out the names of all non-null rasters from the ETD Summary Table\n",
    "resultsTable = kdeRangesAnnual_Path + '\\\\' + 'KDESummaryTable'\n",
    "nonNullKDERasters = []\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        nonNullKDERasters.append(ResultRaster)\n",
    "del cursor\n",
    "print str(len(nonNullKDERasters))\n",
    "\n",
    "#Determine any rasters that are not part of the non-null list\n",
    "arcpy.env.workspace=kdeRangesAnnual_Path\n",
    "rastToDel = []\n",
    "rastCursor = arcpy.ListRasters()\n",
    "for rast in rastCursor:\n",
    "    if str(rast) not in nonNullKDERasters:\n",
    "        rastToDel.append(str(rast))\n",
    "del rastCursor\n",
    "print str(len(rastToDel))\n",
    "\n",
    "#delete these orphaned rasters from the database to clean it up\n",
    "count=0\n",
    "for delRast in rastToDel:\n",
    "    count=count+1\n",
    "    print str(count)\n",
    "    arcpy.Delete_management(delRast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 16-Day KDE Percentile Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the 16-Day KDE Summary Table and loop through the values\n",
    "percentArea = PercentileArea()\n",
    "resultsTable = kdeRanges16Day_Path + '\\\\' + 'KDESummaryTable'\n",
    "gcsSR = FeatureDataHelper.CreateGeographicSR(4326)\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        print ResultRaster\n",
    "        percentAreaParams = PercentileAreaCalculationParameters(percentiles, 1E-20, ResultRaster, kdeRanges16Day_Props,\n",
    "                                                 \"KDE16DayPolygonsGCS\", True, gcsSR, row.getValue(\"MovDataID\"),\n",
    "                                                                row.getValue(\"CalcID\"), kdeRanges16Day_Props)\n",
    "        percentArea.CalculatePercentileAreas(percentAreaParams)\n",
    "del cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Annual KDE Percentile Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Annual KDE Summary Table and loop through the values\n",
    "percentArea = PercentileArea()\n",
    "resultsTable = kdeRangesAnnual_Path + '\\\\' + 'KDESummaryTable'\n",
    "gcsSR = FeatureDataHelper.CreateGeographicSR(4326)\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        print ResultRaster\n",
    "        percentAreaParams = PercentileAreaCalculationParameters(percentiles, 1E-20, ResultRaster, kdeRangesAnnual_Props,\n",
    "                                                 \"KDEAnnualPolygonsGCS\", True, gcsSR, row.getValue(\"MovDataID\"),\n",
    "                                                                row.getValue(\"CalcID\"), kdeRangesAnnual_Props)\n",
    "        percentArea.CalculatePercentileAreas(percentAreaParams)\n",
    "del cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ETD Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ETD Ranges and Temporality Metrics for all elephants\n",
    "t1=datetime.datetime.now()\n",
    "\n",
    "for name in eleNamesArray:\n",
    "    print(\"Analysing: \" + name)\n",
    "    cleanName = FeatureDataHelper.CleanupName(name) + '_Locs'\n",
    "    \n",
    "    #Open the movement data feature class\n",
    "    MovFC = FeatureDataHelper.ReturnFeatureClass2(MovGDB_Props,cleanName)\n",
    "\n",
    "    #Create a trajectory\n",
    "    path = TrajectoryEngine.CreateTrajectoryFromPointFeatureClass(MovFC,'',\"Fixtime\",True)\n",
    "\n",
    "    #Define the CalcID\n",
    "    calcID = cleanName\n",
    "\n",
    "    #Calculate 16-Day Windows\n",
    "    modisDateObject = MODIS16DayWindowUnit(path.StartTime)\n",
    "    windowDates16Day = modisDateObject.DefineWindowTimes(path.StartTime,path.EndTime)\n",
    "\n",
    "    #Calculate Annual Windows\n",
    "    annualWindowObject = YearWindowUnit(path.StartTime,1)\n",
    "    windowDatesAnnual = annualWindowObject.DefineWindowTimes(path.StartTime,path.EndTime,1.0)\n",
    "\n",
    "    #Skip dates where the calc was already run\n",
    "    windowDates16Day = alreadyRun(name,etdRanges16Day_Path,'ETDSummaryTable', windowDates16Day)\n",
    "    windowDatesAnnual = alreadyRun(name,etdRangesAnnual_Path,'ETDSummaryTable', windowDatesAnnual)\n",
    "\n",
    "    #Calculate the ETD 16-Day Range \n",
    "    if windowDates16Day.Count > 0:\n",
    "              \n",
    "        #Provide default values so that parametrization will take place automatically for each window segment\n",
    "        etdWeibullParams = EllipticalTimeDensityKernel2Parameters(1.0,1.0)\n",
    "        \n",
    "        #MaxDataGapSeconds,maxSpeed,maxSpeedPercent,intStepKmHr,WeibullParams,ProbCutOffVal,outRasterName,rasterExpansionRatio,rasterResolution,movDataID\n",
    "        etdParams16Day = EllipticalTimeDensityRangeCalcParameters(43200,0.0,0.90,0.0001,etdWeibullParams,1E-20,cleanName + '_etd',\n",
    "                                                                  1.3,200,name,calcID,path.StartTime,path.EndTime,etdRanges16Day_Props)\n",
    "        #Run the calculation\n",
    "        window.IterateWindow(path,windowDates16Day,False,True,etdRangeFuncDelegate,etdParams16Day)\n",
    "        \n",
    "        #Cleanup\n",
    "        del etdParams16Day,etdWeibullParams\n",
    "\n",
    "    #Calculate the ETD Annual Range\n",
    "    if windowDatesAnnual.Count > 0:\n",
    "        \n",
    "        #Provide default values so that parametrization will take place automatically for each window segment\n",
    "        etdWeibullParams = EllipticalTimeDensityKernel2Parameters(1.0,1.0)\n",
    "        \n",
    "        #MaxDataGapSeconds,maxSpeed,maxSpeedPercent,intStepKmHr,WeibullParams,ProbCutOffVal,outRasterName,rasterExpansionRatio,rasterResolution,movDataID\n",
    "        etdParamsAnnual = EllipticalTimeDensityRangeCalcParameters(43200,0.0,0.90,0.0001,etdWeibullParams,1E-20,cleanName + '_etd',\n",
    "                                                                   1.3,200,name,calcID,path.StartTime,path.EndTime,etdRangesAnnual_Props)\n",
    "        #Run the calculation\n",
    "        window.IterateWindow(path,windowDatesAnnual,False,True,etdRangeFuncDelegate,etdParamsAnnual)\n",
    "        \n",
    "        #Cleanup\n",
    "        del etdParamsAnnual,etdWeibullParams\n",
    "\n",
    "    #Cleanup\n",
    "    del path, modisDateObject, annualWindowObject, windowDates16Day, windowDatesAnnual\n",
    "\n",
    "t2=datetime.datetime.now()\n",
    "deltaT=t2-t1\n",
    "print('The ETD calculation took: ' + str(deltaT.total_seconds()/60) + ' minutes.') #2810.91 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up 16-Day ETD database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out the names of all non-null rasters from the ETD Summary Table\n",
    "resultsTable = etdRanges16Day_Path + '\\\\' + 'ETDSummaryTable'\n",
    "nonNullETDRasters = []\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        nonNullETDRasters.append(ResultRaster)\n",
    "del cursor\n",
    "print str(len(nonNullETDRasters))\n",
    "\n",
    "#Determine any rasters that are not part of the non-null list\n",
    "arcpy.env.workspace=etdRanges16Day_Path\n",
    "rastToDel = []\n",
    "rastCursor = arcpy.ListRasters()\n",
    "for rast in rastCursor:\n",
    "    if str(rast) not in nonNullETDRasters:\n",
    "        rastToDel.append(str(rast))\n",
    "del rastCursor\n",
    "print str(len(rastToDel))\n",
    "\n",
    "#delete these orphaned rasters from the database to clean it up\n",
    "count=0\n",
    "for delRast in rastToDel:\n",
    "    count=count+1\n",
    "    print str(count)\n",
    "    arcpy.Delete_management(delRast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Annual ETD database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out the names of all non-null rasters from the ETD Summary Table\n",
    "resultsTable = etdRangesAnnual_Path + '\\\\' + 'ETDSummaryTable'\n",
    "nonNullETDRasters = []\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        nonNullETDRasters.append(ResultRaster)\n",
    "del cursor\n",
    "print str(len(nonNullETDRasters))\n",
    "\n",
    "#Determine any rasters that are not part of the non-null list\n",
    "arcpy.env.workspace=etdRangesAnnual_Path\n",
    "rastToDel = []\n",
    "rastCursor = arcpy.ListRasters()\n",
    "for rast in rastCursor:\n",
    "    if str(rast) not in nonNullETDRasters:\n",
    "        rastToDel.append(str(rast))\n",
    "del rastCursor\n",
    "print str(len(rastToDel))\n",
    "\n",
    "#delete these orphaned rasters from the database to clean it up\n",
    "count=0\n",
    "for delRast in rastToDel:\n",
    "    count=count+1\n",
    "    print str(count)\n",
    "    arcpy.Delete_management(delRast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 16-Day ETD Percentile Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the 16-Day ETD Summary Table and loop through the values\n",
    "percentArea = PercentileArea()\n",
    "resultsTable = etdRanges16Day_Path + '\\\\' + 'ETDSummaryTable'\n",
    "gcsSR = FeatureDataHelper.CreateGeographicSR(4326)\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        print ResultRaster\n",
    "        percentAreaParams = PercentileAreaCalculationParameters(percentiles, 1E-20, ResultRaster, etdRanges16Day_Props,\n",
    "                                                 \"ETD16DayPolygonsGCS\", True, gcsSR, row.getValue(\"MovDataID\"), row.getValue(\"CalcID\"),\n",
    "                                                 etdRanges16Day_Props)\n",
    "        percentArea.CalculatePercentileAreas(percentAreaParams)\n",
    "del cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Annual ETD Percentile Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the Annual ETD Summary Table and loop through the values\n",
    "percentArea = PercentileArea()\n",
    "resultsTable = etdRangesAnnual_Path + '\\\\' + 'ETDSummaryTable'\n",
    "gcsSR = FeatureDataHelper.CreateGeographicSR(4326)\n",
    "cursor = arcpy.SearchCursor(dataset=resultsTable)\n",
    "for row in cursor:\n",
    "    ResultRaster = row.getValue(\"ResultRaster\")\n",
    "    if ResultRaster != \"\":\n",
    "        print ResultRaster\n",
    "        percentAreaParams = PercentileAreaCalculationParameters(percentiles, 1E-20, ResultRaster, etdRangesAnnual_Props,\n",
    "                                                 \"ETDAnnualPolygonsGCS\", True, gcsSR, row.getValue(\"MovDataID\"),\n",
    "                                                                row.getValue(\"CalcID\"), etdRangesAnnual_Props)\n",
    "        percentArea.CalculatePercentileAreas(percentAreaParams)\n",
    "del cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate temporality metrics\n",
    "t1=datetime.datetime.now()\n",
    "\n",
    "for name in eleNamesArray:\n",
    "    print(\"Analysing: \" + name)\n",
    "    cleanName = FeatureDataHelper.CleanupName(name) + '_Locs'\n",
    "    \n",
    "    #Open the movement data feature class\n",
    "    MovFC = FeatureDataHelper.ReturnFeatureClass2(MovGDB_Props,cleanName)\n",
    "\n",
    "    #Create a trajectory\n",
    "    path = TrajectoryEngine.CreateTrajectoryFromPointFeatureClass(MovFC,'',\"Fixtime\",True)\n",
    "\n",
    "    #Define the CalcID\n",
    "    calcID = cleanName \n",
    "    \n",
    "    #Calculate Modis 16-Day windows\n",
    "    modisDateObject = MODIS16DayWindowUnit(path.StartTime)\n",
    "    windowDates16Day = modisDateObject.DefineWindowTimes(path.StartTime,path.EndTime)\n",
    "\n",
    "    tempMetParams16Day = TemporalMetricsCalcParameters(\"TemporalityMetrics\",True,name,calcID,\n",
    "                                                       path.StartTime,path.EndTime,tempmetrics16Day_Props) \n",
    "    \n",
    "    #Run the 16-Day calculation\n",
    "    window.IterateWindow(path, windowDates16Day,False,True,tempMetFuncDelegate,tempMetParams16Day)\n",
    "    \n",
    "    #Calculate Annual Windows\n",
    "    annualWindowObject = YearWindowUnit(path.StartTime,1)\n",
    "    windowDatesAnnual = annualWindowObject.DefineWindowTimes(path.StartTime,path.EndTime,1.0)\n",
    "\n",
    "    tempMetParamsAnnual = TemporalMetricsCalcParameters(\"TemporalityMetrics\",True,name,calcID,\n",
    "                                                        path.StartTime,path.EndTime,tempmetricsAnnual_Props) \n",
    "    #Run the Annual calculation\n",
    "    window.IterateWindow(path, windowDatesAnnual,False,True,tempMetFuncDelegate,tempMetParamsAnnual)\n",
    "    \n",
    "    #Cleanup\n",
    "    del path, MovFC\n",
    "    del tempMetParams16Day, modisDateObject, windowDates16Day\n",
    "    del tempMetParamsAnnual, annualWindowObject, windowDatesAnnual\n",
    "    gc.collect()\n",
    "\n",
    "t2=datetime.datetime.now()\n",
    "deltaT=t2-t1\n",
    "print('The Temporality Metrics calculation took: ' + str(deltaT.total_seconds()/60) + ' minutes.') #7.06 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Path Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in eleNamesArray:\n",
    "    print(\"Analysing: \" + name)\n",
    "    \n",
    "    cleanName = FeatureDataHelper.CleanupName(name) + '_Locs'\n",
    "    \n",
    "    #Open the movement data feature class\n",
    "    MovFC = FeatureDataHelper.ReturnFeatureClass2(MovGDB_Props,cleanName)\n",
    "\n",
    "    #Create a trajectory\n",
    "    path = TrajectoryEngine.CreateTrajectoryFromPointFeatureClass(MovFC,'',\"Fixtime\",True)\n",
    "\n",
    "    #Define the CalcID\n",
    "    calcID = cleanName \n",
    "    \n",
    "    #Calculate Modis 16-Day windows\n",
    "    modisDateObject = MODIS16DayWindowUnit(path.StartTime)\n",
    "    windowDates16Day = modisDateObject.DefineWindowTimes(path.StartTime, path.EndTime)\n",
    "    \n",
    "    \n",
    "    Params16Day = TrajectoryPathMetricsCalcParameters(\"TrajPathMetrics\", True,  name, calcID,\n",
    "                                                 path.StartTime, path.EndTime, trajPathMetrics16Day_Props) \n",
    "    \n",
    "    #Run the 16-Day calculation\n",
    "    window.IterateWindow(path, windowDates16Day,False,True,trajPathMetricsDelegate,Params16Day)\n",
    "    \n",
    "    #Calculate Annual Windows\n",
    "    annualWindowObject = YearWindowUnit(path.StartTime,1)\n",
    "    windowDatesAnnual = annualWindowObject.DefineWindowTimes(path.StartTime,path.EndTime,1.0)\n",
    "\n",
    "    ParamsAnnual = TrajectoryPathMetricsCalcParameters(\"TrajPathMetrics\", True,  name, calcID,\n",
    "                                                 path.StartTime, path.EndTime, trajPathMetricsAnnual_Props)\n",
    "    #Run the Annual calculation\n",
    "    window.IterateWindow(path, windowDatesAnnual,False,True,trajPathMetricsDelegate,ParamsAnnual)\n",
    "    \n",
    "    #Cleanup\n",
    "    del path, MovFC\n",
    "    del Params16Day, modisDateObject, windowDates16Day\n",
    "    del ParamsAnnual, annualWindowObject, windowDatesAnnual\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join ETD Range Polygons with TrackingMaster, Temporality Metrics, and Regions tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = etdRanges16Day_Path\n",
    "arcpy.JoinField_management(\"ETD16DayPolygonsGCS\", \"CalcID\",\n",
    "                           \"ETDSummaryTable\",\"CalcID\",[\"StartDate\",\"EndDate\",\"WeibullSpeedParams\"])\n",
    "arcpy.JoinField_management(\"ETD16DayPolygonsGCS\", \"MovDataID\",\n",
    "                           tmTable,\"Name\",[\"Chronofile\",\"Species\",\"Sex\",\"Region\",\"Country\",\"MetaRegion\"])\n",
    "arcpy.JoinField_management(\"ETD16DayPolygonsGCS\", \"CalcID\",\n",
    "                           tempmetrics16Day_Path + \"\\\\TemporalityMetrics\",\"CalcID\",[\"NumPoints\",\"TrajTimeHrs\",\"TGI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = etdRangesAnnual_Path\n",
    "arcpy.JoinField_management(\"ETDAnnualPolygonsGCS\", \"CalcID\",\n",
    "                           \"ETDSummaryTable\",\"CalcID\",[\"StartDate\",\"EndDate\",\"WeibullSpeedParams\"])\n",
    "arcpy.JoinField_management(\"ETDAnnualPolygonsGCS\", \"MovDataID\",\n",
    "                           tmTable,\"Name\",[\"Chronofile\",\"Species\",\"Sex\",\"Region\",\"Country\",\"MetaRegion\"])\n",
    "arcpy.JoinField_management(\"ETDAnnualPolygonsGCS\", \"CalcID\",\n",
    "                           tempmetricsAnnual_Path + \"\\\\TemporalityMetrics\",\"CalcID\",[\"NumPoints\",\"TrajTimeHrs\",\"TGI\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protected Area Intersection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Protected Areas (WDPA 2013 - I first manually ran a clip operation of all WDPA polygons falling within Africa to the 4 analysis metaregions and then dissolved them. There were issues with the dissolve operation not working and tool kept producing multiple polygons. I had to run a 'Repair Geometry' operation on them before the Dissolve would work...\n",
    "\n",
    "Tried to use the Esri TabulateIntersect tool:\n",
    "arcpy.TabulateIntersection_analysis(in_zone_features=wpda,zone_fields='OBJECTID',in_class_features=\"MCP_Merge\",\n",
    "class_fields=[\"CalcID\"], out_table=\"WPDA_Intersect\",out_units='SQUARE_KILOMETERS')\n",
    "\n",
    "but I ran into huge problems. First of all, when running all 13K+ polygons, the tool would simply not produce output but would not give any error message. I ran 'Repair Geometry' and that did not help. Might be a problem with a single range but didn't  find it. I tried breaking the ranges up into groups of 2000 and running that way. This worked but some polygons would randomly be left out (e.g., ~1700 intersections would be performed but not 2000 even though it was also recording 0% intersections). Not sure what the problem is there. So instead I used the SpatialIntegrator tool in ArcMET which adds a field to the MCP feature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spInt = SpatialIntegrator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the intersection area of each ETD 16-day range with the protected areas layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16-Day\n",
    "t1=datetime.datetime.now()\n",
    "arcpy.env.workspace = etdRanges16Day_Path\n",
    "spIntCalcParams = SpatialIntegratorFeatureCalcParams(\"WDPApoint_August2013_Clip_Dissolv\",wdpaGDB_Props,'',\n",
    "                                                     'ETD16DayPolygonsGCS','IntersectArea','',etdRanges16Day_Props)\n",
    "spInt.SpatiallyIntegrateByZone(spIntCalcParams)\n",
    "t2=datetime.datetime.now()\n",
    "deltaT=t2-t1\n",
    "print('The 16-Day intersection took: ' + str(deltaT.total_seconds()/60) + ' minutes.') #482.23 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the % intersection of each ETD 16-day range with the protected areas layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a field to hold the percent intersection value\n",
    "arcpy.env.workspace = etdRanges16Day_Path\n",
    "desc = arcpy.Describe('ETD16DayPolygonsGCS')\n",
    "areafieldname = desc.AreaFieldName\n",
    "arcpy.AddField_management(in_table='ETD16DayPolygonsGCS', field_name='IntersectPercent',\n",
    "                          field_type='DOUBLE', field_is_nullable='NULLABLE')\n",
    "\n",
    "#Calculate the percent intersect value\n",
    "cursor = arcpy.UpdateCursor('ETD16DayPolygonsGCS')\n",
    "for row in cursor:\n",
    "    totalArea=row.getValue(areafieldname)\n",
    "    intersectArea=row.getValue('IntersectArea')\n",
    "    if totalArea > 0:\n",
    "        row.setValue('IntersectPercent', intersectArea/totalArea)  \n",
    "    else:\n",
    "        row.setValue('IntersectPercent', 0 )\n",
    "    cursor.updateRow(row)\n",
    "del(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the intersection area of each ETD annual range with the protected areas layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual\n",
    "t1=datetime.datetime.now()\n",
    "arcpy.env.workspace = etdRangesAnnual_Path\n",
    "spIntCalcParams = SpatialIntegratorFeatureCalcParams(\"WDPApoint_August2013_Clip_Dissolv\",wdpaGDB_Props,'',\n",
    "                                                     'ETDAnnualPolygonsGCS','IntersectArea','',etdRangesAnnual_Props)\n",
    "spInt.SpatiallyIntegrateByZone(spIntCalcParams)\n",
    "t2=datetime.datetime.now()\n",
    "deltaT=t2-t1\n",
    "print('The Annual intersection took: ' + str(deltaT.total_seconds()/60) + ' minutes.') #32.6 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the % intersection of each annual range with the protected areas layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a field to hold the percent intersection value\n",
    "arcpy.env.workspace = etdRangesAnnual_Path\n",
    "desc = arcpy.Describe('ETDAnnualPolygonsGCS')\n",
    "areafieldname = desc.AreaFieldName\n",
    "arcpy.AddField_management(in_table='ETDAnnualPolygonsGCS',\n",
    "                          field_name='IntersectPercent', field_type='DOUBLE', field_is_nullable='NULLABLE')\n",
    "\n",
    "#Calculate the percent intersect value\n",
    "cursor = arcpy.UpdateCursor('ETDAnnualPolygonsGCS')\n",
    "for row in cursor:\n",
    "    totalArea=row.getValue(areafieldname)\n",
    "    intersectArea=row.getValue('IntersectArea')\n",
    "    if totalArea > 0:\n",
    "        row.setValue('IntersectPercent', intersectArea/totalArea)  \n",
    "    else:\n",
    "        row.setValue('IntersectPercent', 0 )\n",
    "    cursor.updateRow(row)\n",
    "del(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Covariates for ETD Range Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pandas dataframes from the output ETD polygons feature classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the 90% percentile of the ETD model as per Wall et al. 2014\n",
    "global_where_clause = 'DesiredPercentile=0.90 AND Area > 0.0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertFeatureClassToPandasDataFrame(infc, where_clause):\n",
    "    df = None\n",
    "    try:\n",
    "        convertObj=[]\n",
    "        desc = arcpy.Describe(infc)\n",
    "        \n",
    "        #Shape field name\n",
    "        shapefieldname=\"\"\n",
    "        try:\n",
    "            shapefieldname = desc.ShapeFieldName \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #OID field name\n",
    "        OIDFieldName = desc.OIDFieldName \n",
    "        \n",
    "        # all field names\n",
    "        field_names = [field.name for field in arcpy.ListFields(infc)]\n",
    "        field_names.remove(shapefieldname)\n",
    "\n",
    "        for row in arcpy.SearchCursor(infc, where_clause, field_names):\n",
    "            curRow=[]\n",
    "            for field in field_names:\n",
    "                if field != shapefieldname:\n",
    "                    curRow.append(row.getValue(field))\n",
    "#                 else:\n",
    "#                     curRow.append(ConvertArcGISPolygonToEE_MultiPolygon(row.getValue(field)))\n",
    "            convertObj.append(curRow)\n",
    "        \n",
    "        df = pd.DataFrame(convertObj)\n",
    "        df.columns = field_names\n",
    "\n",
    "    except Exception as e:\n",
    "        s = str(e)\n",
    "        print(s)\n",
    "    finally:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day = ConvertFeatureClassToPandasDataFrame(etdRanges16Day_Path + \"\\\\\" + 'ETD16DayPolygonsGCS',\n",
    "                                               where_clause = global_where_clause)\n",
    "df16Day.index = df16Day.OBJECTID\n",
    "print(len(df16Day.index)) #12467\n",
    "df16Day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual = ConvertFeatureClassToPandasDataFrame(etdRangesAnnual_Path + \"\\\\\" + 'ETDAnnualPolygonsGCS',\n",
    "                                               where_clause = global_where_clause)\n",
    "dfAnnual.index = dfAnnual.OBJECTID\n",
    "print(len(dfAnnual.index)) #755\n",
    "dfAnnual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare against old dataframe\n",
    "# df16Day = pd.read_csv(filepath_or_buffer=etd16DayAnalysisTable,header=0)\n",
    "# print(len(df16Day.index)) #49904\n",
    "# df16Day = df16Day[df16Day.Area > 0.0]\n",
    "# print(len(df16Day.index))\n",
    "# df16Day = df16Day[df16Day.DesiredPercentile == 0.90]\n",
    "# print(len(df16Day.index))\n",
    "# df16Day.index = df16Day.OBJECTID\n",
    "# df16Day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GEE extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Earth Engine object, using your authentication credentials.\n",
    "ee.Initialize()\n",
    "print \"GEE Initialized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the stack reducer\n",
    "imgReduce = ee.Reducer.mean()\n",
    "\n",
    "#Define the region reducer\n",
    "polyReduce = ee.Reducer.mean()\n",
    "\n",
    "#Define the area reducer scale\n",
    "reduceScale = 1000 #meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16-Day\n",
    "def Extract16Day(tempFeat):\n",
    "    start = tempFeat.get('startDate')\n",
    "    end = tempFeat.get('endDate')\n",
    "    end1 = tempFeat.get('endDatePlusOne')\n",
    "\n",
    "    #NDVI\n",
    "    ndviReduce = ee.ImageCollection('MODIS/MCD43A4_NDVI'). \\\n",
    "    select('NDVI').filterDate(start,end1).reduce(imgReduce).\\\n",
    "    reduceRegion(polyReduce, tempFeat.geometry(),reduceScale)\n",
    "\n",
    "    #NDWI\n",
    "    ndwiReduce = ee.ImageCollection('MODIS/MCD43A4_NDWI').\\\n",
    "    select('NDWI').filterDate(start,end1).reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "\n",
    "    #EVI\n",
    "    eviReduce = ee.ImageCollection('MODIS/MCD43A4_EVI').\\\n",
    "    select('EVI').filterDate(start,end1).reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "\n",
    "    #LST\n",
    "    def calibrateLST(image):\n",
    "        return image.multiply(ee.Image(0.02)).subtract(ee.Image(273.15)) # band has a 0.02 scale\n",
    "    lstColl = ee.ImageCollection('MODIS/MOD11A2').filterDate(start,end).select('LST_Day_1km').map(calibrateLST)\n",
    "    tempReduce = lstColl.reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "      \n",
    "    #Tree\n",
    "    treeReduce = ee.ImageCollection('MODIS/051/MOD44B').\\\n",
    "    select('Percent_Tree_Cover').reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "\n",
    "    #TRMM - Note that we use all images in the stack since they are more frequent than the 16-Day\n",
    "    trmmReduce = ee.ImageCollection('TRMM/3B42').\\\n",
    "    select(['precipitation']).filterDate(start,end).reduce(ee.Reducer.sum()).\\\n",
    "    reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "\n",
    "    #Slope\n",
    "    slopeReduce = ee.Algorithms.Terrain(ee.Image('USGS/SRTMGL1_003')).\\\n",
    "    select(['slope']).reduceRegion(polyReduce, tempFeat.geometry(), reduceScale)\n",
    "\n",
    "    #HF\n",
    "    hfImg = ee.Image('users/walljcg/hf_wcs_2009')\n",
    "    hfMasked = hfImg.updateMask(hfImg.gte(0)) #Need to mask out NoData Values = -9999\n",
    "    hfReduce = hfMasked.reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #Water\n",
    "    waterReduce = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').\\\n",
    "    select('occurrence').unmask(0).reduceRegion(polyReduce, tempFeat.geometry(), reduceScale)\n",
    "\n",
    "    return tempFeat.set({\n",
    "        'ndviVal':ndviReduce,\n",
    "        'ndwiVal':ndwiReduce,\n",
    "        'eviVal':eviReduce,\n",
    "        'tempVal':tempReduce, \n",
    "        'treeVal':treeReduce,\n",
    "        'trmmVal':trmmReduce,\n",
    "        'slopeVal':slopeReduce,\n",
    "        'hfVal':hfReduce,\n",
    "        'waterVal': waterReduce})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual\n",
    "def ExtractAnnual(tempFeat):\n",
    "    \n",
    "    start = tempFeat.get('startDate')\n",
    "    end = tempFeat.get('endDate')\n",
    "    \n",
    "    #NDVI\n",
    "    ndviReduce = ee.ImageCollection('MODIS/MCD43A4_006_NDVI').\\\n",
    "    select('NDVI').filterDate(start,end).reduce(imgReduce).reduceRegion(polyReduce, tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #NDWI\n",
    "    ndwiReduce = ee.ImageCollection('MODIS/MCD43A4_006_NDWI').\\\n",
    "    select('NDWI').filterDate(start,end).reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #EVI\n",
    "    eviReduce = ee.ImageCollection('MODIS/MCD43A4_006_EVI').\\\n",
    "    select('EVI').filterDate(start,end).reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #LST\n",
    "    def calibrateLST(image):\n",
    "        return image.multiply(ee.Image(0.02)).subtract(ee.Image(273.15)) # band has a 0.02 scale\n",
    "    lstColl = ee.ImageCollection('MODIS/006/MOD11A2').filterDate(start,end).select('LST_Day_1km').map(calibrateLST)\n",
    "    tempReduce = lstColl.reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #Tree\n",
    "    treeReduce = ee.ImageCollection('MODIS/051/MOD44B').\\\n",
    "    select('Percent_Tree_Cover').reduce(imgReduce).reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #TRMM\n",
    "    trmmReduce = ee.ImageCollection('TRMM/3B42').\\\n",
    "    select(['precipitation']).filterDate(start,end).reduce(ee.Reducer.sum()).\\\n",
    "    reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #Slope\n",
    "    slopeReduce = ee.Algorithms.Terrain(ee.Image('USGS/SRTMGL1_003')).\\\n",
    "    select(['slope']).reduceRegion(polyReduce, tempFeat.geometry(),reduceScale)\n",
    "    \n",
    "    #HF\n",
    "    hfImg = ee.Image('users/walljcg/hf_wcs_2009')\n",
    "    hfMasked = hfImg.updateMask(hfImg.gte(0)) #Need to mask out NoData Values = -9999\n",
    "    hfReduce = hfMasked.reduceRegion(polyReduce,tempFeat.geometry(),reduceScale)\n",
    "\n",
    "    #Water\n",
    "    waterReduce = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').\\\n",
    "    select('occurrence').unmask(0).reduceRegion(polyReduce, tempFeat.geometry(), reduceScale)\n",
    "\n",
    "    return tempFeat.set({'ndviVal':ndviReduce,\n",
    "                         'ndwiVal':ndwiReduce,\n",
    "                         'eviVal':eviReduce,\n",
    "                         'tempVal':tempReduce, \n",
    "                         'treeVal':treeReduce,\n",
    "                         'trmmVal':trmmReduce,\n",
    "                         'slopeVal':slopeReduce,\n",
    "                         'hfVal':hfReduce,\n",
    "                         'waterVal': waterReduce})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractCovariates(infc, extract_func, where_clause):\n",
    "    results = []\n",
    "    t1=dt.datetime.now()\n",
    "    counter = 0\n",
    "\n",
    "    desc = arcpy.Describe(infc)\n",
    "    \n",
    "    OIDFieldName = desc.OIDFieldName\n",
    "    shapefieldname = desc.ShapeFieldName\n",
    "    field_names = [OIDFieldName,shapefieldname]\n",
    "\n",
    "    for row in arcpy.SearchCursor(infc, where_clause, field_names):\n",
    "        try:\n",
    "            curRow=dict()\n",
    "            for name in field_names:\n",
    "                if name != shapefieldname:\n",
    "                    curRow[name]=row.getValue(name)\n",
    "                else:\n",
    "                    rings = ConvertArcGISPolygonToEE_MultiPolygon(row.getValue(name))\n",
    "                    poly = ee.Geometry.MultiPolygon(rings)\n",
    "                    GEEfeat = ee.Feature(poly,{'startDate':convertDateTimeToUnixTime(row.getValue('StartDate')),\n",
    "                                       'endDate': convertDateTimeToUnixTime(row.getValue('EndDate')),\n",
    "                                       'endDatePlusOne': convertDateTimeToUnixTime(row.getValue('StartDate') +\n",
    "                                                                                   dt.timedelta(seconds=1))})\n",
    "                    success = False\n",
    "                    single_result = None\n",
    "                    while success==False:\n",
    "                        try:\n",
    "                            single_result = ee.FeatureCollection(GEEfeat).map(extract_func).getInfo()\n",
    "                            success = True\n",
    "                        except Exception, e:\n",
    "                            print str(e)\n",
    "                            print('An ee error occurred...re-trying...')\n",
    "                            time.sleep(5)  #Pause for 5 seconds before trying again\n",
    "\n",
    "                    vals = single_result['features'][0]['properties']\n",
    "                    curRow['ndviVal']=vals.get('ndviVal', np.nan).get('NDVI_mean', np.nan)\n",
    "                    curRow['ndwiVal']=vals.get('ndwiVal', np.nan).get('NDWI_mean', np.nan)\n",
    "                    curRow['eviVal']=vals.get('eviVal', np.nan).get('EVI_mean', np.nan)\n",
    "                    curRow['tempVal']=vals.get('tempVal', np.nan).get('LST_Day_1km_mean', np.nan)\n",
    "                    curRow['treeVal']=vals.get('treeVal', np.nan).get('Percent_Tree_Cover_mean', np.nan)\n",
    "                    curRow['trmmVal']=vals.get('trmmVal', np.nan).get('precipitation_sum', np.nan)\n",
    "                    curRow['hfVal']=vals.get('hfVal', np.nan).get('b1', np.nan)\n",
    "                    curRow['slopeVal']=vals.get('slopeVal', np.nan).get('slope', np.nan)\n",
    "                    curRow['waterVal']=vals.get('waterVal', np.nan).get('occurrence', np.nan)\n",
    "            \n",
    "            results.append(curRow)\n",
    "            counter = counter + 1\n",
    "            print(str(counter) + ': ' + str(curRow))\n",
    "        except Exception as e:\n",
    "            print str(e)\n",
    "    t2=dt.datetime.now()\n",
    "    deltaT=t2-t1\n",
    "    print('The covariate extraction took: ' + str(deltaT.total_seconds()/60) + ' minutes.') # 263.6 minutes\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16DayCovars = pd.DataFrame(ExtractCovariates(etdRanges16Day_Path + \"\\\\\" + 'ETD16DayPolygonsGCS',\n",
    "                                               Extract16Day, global_where_clause))\n",
    "df16DayCovars.index = df16DayCovars.OBJECTID\n",
    "print(len(df16DayCovars.index))\n",
    "df16DayCovars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "df16Day = df16Day.merge(df16DayCovars)\n",
    "print(len(df16Day.index))\n",
    "df16Day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.drop('SHAPE_Length',axis=1, inplace=True)\n",
    "df16Day.drop('SHAPE_Area',axis=1, inplace=True)\n",
    "df16Day.drop('SHAPE',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.to_csv(path_or_buf=etd16DayAnalysisTable + '_new',\n",
    "             date_format='%Y-%m-%d %H:%M:%S', index=False, na_rep='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Annual Covariate Info using GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnualCovars = pd.DataFrame(ExtractCovariates(etdRangesAnnual_Path + \"\\\\\" + 'ETDAnnualPolygonsGCS',\n",
    "                                               ExtractAnnual, global_where_clause))\n",
    "dfAnnualCovars.index = dfAnnualCovars.OBJECTID\n",
    "print(len(dfAnnualCovars.index))\n",
    "dfAnnualCovars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "dfAnnual = dfAnnual.merge(dfAnnualCovars)\n",
    "print(len(dfAnnual.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.drop('SHAPE_Length',axis=1, inplace=True)\n",
    "dfAnnual.drop('SHAPE_Area',axis=1, inplace=True)\n",
    "dfAnnual.drop('SHAPE',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.to_csv(path_or_buf=etdAnnualAnalysisTable + '_new',\n",
    "             date_format='%Y-%m-%d %H:%M:%S',index=False,na_rep='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up the 16-Day dataframe and re-export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day = pd.read_csv(filepath_or_buffer=etd16DayAnalysisTable +  '_new', header=0)\n",
    "print(len(df16Day.index)) #12467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a custom filter to remove ranges for subject Taurus who was confined by a fence for different periods\n",
    "def filterTaurusRanges(nm,testDates, t1, t2):\n",
    "    result = False\n",
    "    if nm=='Taurus':\n",
    "        for i in testDates:\n",
    "            if (i >= t1) & (i <= t2):\n",
    "                result = True\n",
    "    return result\n",
    "\n",
    "taurusIntersectionDates = pd.read_csv(filepath_or_buffer=ignoreDataFile, header=0,\n",
    "                                      usecols=['MovDataID','Fixtime'],parse_dates=['Fixtime'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df16Day.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporarily change the water NA values to 0's\n",
    "df16Day.waterVal.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the range of values\n",
    "colsToCheck = ['ndviVal','ndwiVal','eviVal','tempVal','treeVal','trmmVal','hfVal','slopeVal', 'waterVal']\n",
    "def valsRange(x):\n",
    "    return [x.min(),x.max()]\n",
    "for i in colsToCheck:\n",
    "    print(i +': ' + str(valsRange(df16Day[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day = df16Day.dropna() #Remove any rows that contain NA values\n",
    "print(len(df16Day.index)) # 11980\n",
    "\n",
    "#Convert to Sq.Km from m^2\n",
    "df16Day[\"AreaKm\"]=df16Day[\"Area\"]/1000000\n",
    "\n",
    "#Select the 90% percentile of the ETD model as per Wall et al. 2014\n",
    "df16Day=df16Day[df16Day[\"DesiredPercentile\"]==0.90]\n",
    "print(len(df16Day.index)) # 11980\n",
    "\n",
    "#Convert the start/end time columns to datetime objects and append to the dataframe\n",
    "df16Day['StartDate2']=pd.to_datetime(df16Day['StartDate'])\n",
    "df16Day['EndDate2']=pd.to_datetime(df16Day['EndDate'])\n",
    "\n",
    "#Delete the Range areas known to be wrong \n",
    "df16Day['filterRange'] = False #Create a column with each value false\n",
    "df16Day=df16Day[~((df16Day['MovDataID']=='Chukwi') &\n",
    "          (df16Day['EndDate2'] > dt.datetime.strptime('2013-07-23 15:00:06',\"%Y-%m-%d %H:%M:%S\")))]\n",
    "df16Day=df16Day[~((df16Day['MovDataID']=='Kuku') &\n",
    "          (df16Day['EndDate2'] > dt.datetime.strptime('2013-08-24 14:22:23',\"%Y-%m-%d %H:%M:%S\")))]\n",
    "df16Day=df16Day[~((df16Day['MovDataID']=='Winston') &\n",
    "          (df16Day['EndDate2'] > dt.datetime.strptime('2003-12-19 03:00:56',\"%Y-%m-%d %H:%M:%S\")))]\n",
    "\n",
    "#filter ranges that intersect times that Taurus was in the army camp\n",
    "df16Day['filterRange'] = df16Day.apply(lambda x: \n",
    "                                      filterTaurusRanges(nm=x['MovDataID'],\n",
    "                                      testDates = pd.to_datetime(taurusIntersectionDates['Fixtime']),\n",
    "                                      t1 = pd.to_datetime(x['StartDate']),\n",
    "                                      t2 = pd.to_datetime(x['EndDate'])),axis=1)\n",
    "df16Day=df16Day[df16Day['filterRange']==False]\n",
    "\n",
    "# drop the small home-range for the elephant Nuri where there was a mortality during the last two week period of the data\n",
    "df16Day = df16Day.drop(df16Day[(df16Day['MovDataID']=='Nuri') & (df16Day['EndDate']>='2003-09-26 08:00:00')].index)\n",
    "\n",
    "print(len(df16Day.index)) # 11,901\n",
    "\n",
    "#Here we limit the data to anything with a TGI value equivalent to 12-hour sampling\n",
    "#16 days = 16*24 = 384 hours\n",
    "#There are m=(384 hours)/(12 hours)=32 12-hour segments in a 16-day period\n",
    "#The annual TGI value for 12 hour sampling would be 1/m=1/32=0.03125\n",
    "\n",
    "TGI_Cutoff=0.03125\n",
    "\n",
    "#Filter ranges based on TGI scores\n",
    "df16Day=df16Day[df16Day['TGI'] < TGI_Cutoff]\n",
    "print(len(df16Day.index)) #10,319\n",
    "\n",
    "df16Day.to_csv(path_or_buf=etd16DayAnalysisTableFilt + '_new', date_format='%Y-%m-%d %H:%M:%S', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up the Annual dataframe and re-export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual = pd.read_csv(filepath_or_buffer=etdAnnualAnalysisTable + '_new', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily change the water NA values to 0's\n",
    "dfAnnual.waterVal.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the range of values\n",
    "colsToCheck = ['ndviVal','ndwiVal','eviVal','tempVal','treeVal','trmmVal','hfVal','slopeVal', 'waterVal']\n",
    "def valsRange(x):\n",
    "    return [x.min(),x.max()]\n",
    "for i in colsToCheck:\n",
    "    print(i +': ' + str(valsRange(dfAnnual[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual = dfAnnual.dropna() # Remove any rows that contain NA values\n",
    "print(len(dfAnnual.index)) # 2968\n",
    "\n",
    "#Convert to Sq.Km from m^2\n",
    "dfAnnual[\"AreaKm\"]=dfAnnual[\"Area\"]/1000000\n",
    "\n",
    "#Select the 90% percentile of the ETD model as per Wall et al. 2014\n",
    "dfAnnual=dfAnnual[dfAnnual[\"DesiredPercentile\"]==0.90]\n",
    "print(len(dfAnnual.index)) # 742\n",
    "\n",
    "#Convert the start/end time columns to datetime objects and append to the dataframe\n",
    "dfAnnual['StartDate2']=pd.to_datetime(dfAnnual['StartDate'])\n",
    "dfAnnual['EndDate2']=pd.to_datetime(dfAnnual['EndDate'])\n",
    "\n",
    "#Delete the Range areas known to be wrong (note - might still delete the times that Taurus was in the army camp?)\n",
    "dfAnnual=dfAnnual[~((dfAnnual['MovDataID']=='Chukwi') &\n",
    "          (dfAnnual['EndDate2'] > dt.datetime.strptime('2013-07-23 15:00:06',\"%Y-%m-%d %H:%M:%S\")))]\n",
    "dfAnnual=dfAnnual[~((dfAnnual['MovDataID']=='Kuku') &\n",
    "          (dfAnnual['EndDate2'] > dt.datetime.strptime('2013-08-24 14:22:23',\"%Y-%m-%d %H:%M:%S\")))]\n",
    "\n",
    "\n",
    "#filter ranges that intersect times that Taurus was in the army camp\n",
    "dfAnnual['filterRange'] = dfAnnual.apply(lambda x: \n",
    "                                      filterTaurusRanges(nm=x['MovDataID'],\n",
    "                                      testDates = pd.to_datetime(taurusIntersectionDates['Fixtime']),\n",
    "                                      t1 = pd.to_datetime(x['StartDate']),\n",
    "                                      t2 = pd.to_datetime(x['EndDate'])),axis=1)\n",
    "dfAnnual=dfAnnual[dfAnnual['filterRange']==False]\n",
    "print(len(dfAnnual.index)) # 738\n",
    "\n",
    "#Here we limit the data to anything with a TGI value equivalent to 12-hour sampling\n",
    "#There are m=(8760 hours)/(12 hours)=730 24-hour segments in a year\n",
    "#There are m=(8760 hours)/(24 hours)=365 24-hour segments in a year\n",
    "#There are m=(8760 hours)/(36 hours)=243.33 36-hour segments in a year\n",
    "#There are m=(8760 hours)/(48 hours)=182.5 48-hour segments in a year\n",
    "#The annual TGI value for 12 hour sampling would be 1/m=1/730=0.00136\n",
    "#The annual TGI value for 24 hour sampling would be 1/m=1/365=0.0027\n",
    "#The annual TGI value for 36 hour sampling would be 1/m=1/243.33=0.0041\n",
    "#The annual TGI value for 48 hour sampling would be 1/m=1/182.5=0.00547\n",
    "\n",
    "TGI_Cutoff=0.00136\n",
    "\n",
    "#Filter ranges based on TGI scores\n",
    "dfAnnual=dfAnnual[dfAnnual['TGI'] < TGI_Cutoff]\n",
    "print(len(dfAnnual.index)) # 302\n",
    "\n",
    "dfAnnual.to_csv(path_or_buf=etdAnnualAnalysisTableFilt + '_new', date_format='%Y-%m-%d %H:%M:%S', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = MovGDB_Path\n",
    "\n",
    "totalCount=0\n",
    "\n",
    "datasetSummaryTable=[]\n",
    "\n",
    "for fc in arcpy.ListFeatureClasses(\"*_Locs\",feature_type=\"Point\"):\n",
    "    try:\n",
    "        #Need to get the unique list of chronofiles for the given animal using the values in the CalcID column\n",
    "        chronofiles = list(set([newrow[0] for newrow in arcpy.da.SearchCursor(fc,\"CalcID\",where_clause=\"\")]))\n",
    "        #print fc\n",
    "        #print(chronofiles)\n",
    "        for chrono in chronofiles:\n",
    "            #Query the records for the given chronofile\n",
    "            sqlString =\"ORDER BY Fixtime ASC\"\n",
    "            fixtimes = [newrow[0] for newrow in arcpy.da.SearchCursor(fc,\"Fixtime\",\n",
    "                                                                      where_clause=\"\\\"CalcID\\\"='\" + chrono + \"'\",\n",
    "                                                                      sql_clause=(None,sqlString))]\n",
    "            \n",
    "            #Record the number of fixes\n",
    "            #print(str(len(fixtimes)))\n",
    "            totalCount=totalCount+len(fixtimes)\n",
    "\n",
    "            #Calculate the n-1 timespans from the fixtimes\n",
    "            deltaTimes=[]\n",
    "            for j in range(1,len(fixtimes)):\n",
    "                deltaTime = fixtimes[j]-fixtimes[j-1]\n",
    "                deltaTimes.append(deltaTime.total_seconds())\n",
    "\n",
    "            #Determine the top 3 most frequent time-span lengths (in minutes)\n",
    "            if len(deltaTimes) > 0: \n",
    "                #Bins: start at 2.5 mins then go in 5 minute increments upto 48 hours\n",
    "                hist, bin_edges=np.histogram(deltaTimes,bins=range(150,172800,300))  \n",
    "                histInd = np.argsort(hist,kind='mergesort')\n",
    "                val1Min = bin_edges[histInd[len(histInd)-1]]\n",
    "                val1Max = bin_edges[histInd[len(histInd)-1]+1]\n",
    "                val2Min = bin_edges[histInd[len(histInd)-2]]\n",
    "                val2Max = bin_edges[histInd[len(histInd)-2]+1]\n",
    "                val3Min = bin_edges[histInd[len(histInd)-3]]\n",
    "                val3Max = bin_edges[histInd[len(histInd)-3]+1]\n",
    "                meanVal1=(val1Min+val1Max)/2\n",
    "                meanVal2=(val2Min+val2Max)/2\n",
    "                meanVal3=(val3Min+val3Max)/2\n",
    "\n",
    "                #Get the first and last fix\n",
    "                firstFix=fixtimes[0]\n",
    "                lastFix=fixtimes[len(fixtimes)-1]\n",
    "\n",
    "                #Calculate the Data Duration\n",
    "                dataDuration=lastFix-firstFix\n",
    "                dataDurationDays=dataDuration.total_seconds()/float(86400)\n",
    "\n",
    "                #Determine what percentage of the data falls within the given bin\n",
    "                count1=0\n",
    "                count2=0\n",
    "                count3=0\n",
    "                for k in range(len(deltaTimes)):\n",
    "                    if val1Max > deltaTimes[k] >= val1Min:\n",
    "                        count1=count1+deltaTimes[k]\n",
    "                    elif val2Max > deltaTimes[k] >= val2Min:\n",
    "                        count2=count2+deltaTimes[k]\n",
    "                    elif val3Max > deltaTimes[k] >= val3Min:\n",
    "                        count3=count3+deltaTimes[k]         \n",
    "                val1Percent = (count1/float(dataDuration.total_seconds()))*100\n",
    "                if val1Percent < 1:#Set-Null if less than 1% of data\n",
    "                    val1Percent=0\n",
    "                    meanVal1=0 \n",
    "                val2Percent = (count2/float(dataDuration.total_seconds()))*100\n",
    "                if val2Percent < 1:#Set-Null if less than 1% of data\n",
    "                    val2Percent=0\n",
    "                    meanVal2=0\n",
    "                val3Percent = (count3/float(dataDuration.total_seconds()))*100\n",
    "                if val3Percent < 1:#Set-Null if less than 1% of data\n",
    "                    val3Percent=0\n",
    "                    meanVal3=0\n",
    "\n",
    "                freqStats=[[meanVal1,val1Percent],[meanVal2,val2Percent],[meanVal3,val3Percent]]\n",
    "                freqStats.sort(reverse=True, key=lambda tup: tup[1])\n",
    "\n",
    "                #Setup the data_starts and data_stops output strings\n",
    "                startString=\"\"\n",
    "                if not firstFix==None:\n",
    "                    startString=firstFix.strftime(\"%Y-%m-%d %H:%M\") #this gets the first valid datapoint\n",
    "                endString=\"\"\n",
    "                if not lastFix==None:\n",
    "                    endString=lastFix.strftime(\"%Y-%m-%d %H:%M\") #this gets the last valid datapoint\n",
    "\n",
    "                outRow=[str(fc),int(chrono),startString,dataDurationDays,freqStats[0][0]/60,len(fixtimes)]\n",
    "                datasetSummaryTable.append(outRow)\n",
    "    except:\n",
    "        print('An error occurred...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the calculated table above with the trackingmaster table\n",
    "#Explicitly, set the correct datatypes for anything other than strings because the conversion from the ArcGIS types to Python types\n",
    "#doesn't work very well and they end up as 'Objects' rather than floats, ints etc. and this leads to some weird results later\n",
    "\n",
    "datasetSummaryDF=pd.DataFrame(datasetSummaryTable)\n",
    "datasetSummaryDF.columns = [\"FC\",\"Chronofile\",\"DataStarts\",\"Duration_Days\",\"SampFreqMode\",\"DataPointCount\"]\n",
    "datasetSummaryDF['Chronofile'] = datasetSummaryDF['Chronofile'].astype(int)\n",
    "datasetSummaryDF['DataStarts']=pd.to_datetime(datasetSummaryDF['DataStarts'])\n",
    "#print datasetSummaryDF.head()\n",
    "\n",
    "tmColumns=['Chronofile','Collar_type','Name','Species','Sex','Region','Country','MetaRegion']\n",
    "tmDF = pd.DataFrame([row for row in arcpy.da.SearchCursor(tmTable, tmColumns)])\n",
    "tmDF.columns = tmColumns\n",
    "tmDF['Chronofile']=tmDF['Chronofile'].astype(int)\n",
    "#print tmDF.head()\n",
    "\n",
    "#Perform an inner join\n",
    "datasetSummaryDF=pd.merge(datasetSummaryDF,tmDF,on='Chronofile',how='inner')\n",
    "\n",
    "#Add a column with just a bunch of ones - useful for pivot table summaries\n",
    "datasetSummaryDF['DS']=np.full((len(datasetSummaryDF.index),1),1,dtype=int)\n",
    "\n",
    "#Display first few columns\n",
    "print datasetSummaryDF.head()\n",
    "\n",
    "#Print info about the dataframe\n",
    "#datasetSummaryDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the datasetSummaryDF to a csv\n",
    "datasetSummaryDF.to_csv(tablesFolder + '\\\\Dataset_Summary.csv', index=False, columns=datasetSummaryDF.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasetSummaryDF\n",
    "datasetSummaryDF = pd.read_csv(tablesFolder + '\\\\Dataset_Summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetSummaryDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject and dataset count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique individuals to start with \n",
    "print len(list(set(datasetSummaryDF['Name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of datasets\n",
    "print sum(datasetSummaryDF['DS'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of individuals used in range analyses\n",
    "print len(list(set(df16Day['MovDataID'])))\n",
    "print len(list(set(dfAnnual['MovDataID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total datapoint count\n",
    "print datasetSummaryDF['DataPointCount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median and IQR of individual positions\n",
    "datasetSummaryDF.groupby('Name')['DataPointCount'].agg(np.sum).describe()\n",
    "#IQR = 19173 - 2245 = 16928\n",
    "#Median = 7417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Count by Sex within Region, Country, Meta Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=datasetSummaryDF.pivot_table(index=['MetaRegion','Country','Region'],\n",
    "                                 columns=['Sex'],\n",
    "                                 values=['Name'],\n",
    "                                 aggfunc=[lambda x: len(x.unique())],\n",
    "                                 fill_value=0)\n",
    "\n",
    "df1.fillna(value=0, inplace=True)\n",
    "df1.to_csv(tablesFolder + '\\\\Subject_Count_by_Sex_within_Region.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datapoint Count by Sex within Region, Country, Meta Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=datasetSummaryDF.pivot_table(index=['MetaRegion','Country','Region'],\n",
    "                                 columns=['DS'],\n",
    "                                 values=['DataPointCount'],\n",
    "                                 aggfunc=[sum],\n",
    "                                 fill_value=0)\n",
    "\n",
    "df1.fillna(value=0, inplace=True)\n",
    "df1.to_csv(tablesFolder + '\\\\DataPoint_Count_by_Sex_within_Region.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collar Type Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collar-Model-Summary Table in the Appendix\n",
    "collarTypeSummaryDF=datasetSummaryDF.pivot_table(index=['Collar_type'], values=['Chronofile'],\n",
    "                                                 aggfunc=lambda x: x.count(),fill_value=0,margins=False)\n",
    "collarTypeSummaryDF.columns = ['Dataset Count']\n",
    "collarTypeSummaryDF.to_csv(tablesFolder + '\\\\Collar-Tech-Summary.csv')\n",
    "collarTypeSummaryDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Sampling Frequency Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the median dataset duration?\n",
    "datasetSummaryDF['Duration_Days'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of individuals with over a year of data\n",
    "dfgrp = datasetSummaryDF.groupby('Name')['Duration_Days'].agg(np.sum).to_frame()\n",
    "dfgrp[dfgrp['Duration_Days']>=730]['Duration_Days'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many datasets are there at each of the sampling frquencies\n",
    "#And\n",
    "#What is the median dataset duration for the different factor levels of SampFreq Mode?\n",
    "#And\n",
    "#What is the maximum dataset duration for the different factor levels of SampFreq Mode?\n",
    "\n",
    "df1=datasetSummaryDF.pivot_table(index=['SampFreqMode'],values=['Duration_Days'],\n",
    "                                 aggfunc=[lambda x:x.count(),np.median,np.max],fill_value=0)\n",
    "df1.columns=['Dataset Count','Median Duration (Days)', 'Max Duration']\n",
    "df1.to_csv(tablesFolder + '\\\\DataSet_Sampling_Summary.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collar Deployment Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the first fix location from the supplied filtered & merged dataset\n",
    "\n",
    "#Create a new blank feature class to hold the output\n",
    "arcpy.env.workspace = spatialGDB_Path\n",
    "\n",
    "collarDeploymentLocsFC = 'CollarDeploymentLocations'\n",
    "\n",
    "if not arcpy.Exists(collarDeploymentLocsFC):\n",
    "    arcpy.CreateFeatureclass_management(spatialGDB_Path, collarDeploymentLocsFC,\n",
    "                                        \"POINT\", MovGDB_Path + \"\\\\MergeAllLocsGCS\", \"DISABLED\", \"DISABLED\", wgs84GCS) \n",
    "\n",
    "#Get an insert cursor on the CollarDeploymentLocations feature class\n",
    "insertCursor = arcpy.InsertCursor(collarDeploymentLocsFC)\n",
    "\n",
    "for i in eleNamesArray:\n",
    "    print \"Name: \" + i\n",
    "    #Get the list of unique name_chrono vals for a given name\n",
    "    queryString=\"MovDataID='\" + i + \"'\"\n",
    "    chronos=list(set([newrow[0] for newrow in arcpy.da.SearchCursor(MovGDB_Path + \"\\\\MergeAllLocsGCS\",[\"CalcID\"],\n",
    "                                                                    where_clause=queryString)]))\n",
    "    for j in chronos:\n",
    "        #Now go through each chrono value\n",
    "        queryString2=\"CalcID='\" + j + \"'\"\n",
    "        sqlPostfixString =\"ORDER BY Fixtime ASC\"\n",
    "        fixes = [newrow for newrow in arcpy.da.SearchCursor(MovGDB_Path + \"\\\\MergeAllLocsGCS\",[\"CalcID\",\"Fixtime\",\"SHAPE@\"],\n",
    "                                                            where_clause=queryString2,sql_clause=(\"\",sqlPostfixString))]\n",
    "        if len(fixes) != 0:\n",
    "            #print(len(fixes))\n",
    "            firstFix=fixes[0]\n",
    "            feat = insertCursor.newRow()\n",
    "            feat.setValue(\"MovDataID\", i)\n",
    "            feat.setValue(\"CalcID\", firstFix[0])\n",
    "            feat.setValue(\"Fixtime\", firstFix[1])\n",
    "            feat.shape=firstFix[2]\n",
    "            insertCursor.insertRow(feat)\n",
    "        del(fixes)\n",
    "    del(chronos)\n",
    "del(insertCursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Range Size Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 90th percentile ETD Data\n",
    "df16Day = pd.read_csv(filepath_or_buffer=etd16DayAnalysisTableFilt + '_new', header=0, parse_dates=True)\n",
    "dfAnnual = pd.read_csv(filepath_or_buffer=etdAnnualAnalysisTableFilt + '_new', header=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of 16-Day ETD 90th percentile Ranges')\n",
    "sns.distplot(df16Day.AreaKm, ax=ax)\n",
    "plt.savefig(figuresFolder + r'/distributions/16Day_Range_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD 90th percentile Ranges')\n",
    "sns.distplot(dfAnnual.AreaKm)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_Range_Dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean 16-day Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_16day_range = np.mean(df16Day.AreaKm)\n",
    "mean_16day_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent square pixel cell dimension\n",
    "np.sqrt(mean_16day_range * 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum 16 Day Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.iloc[df16Day[\"AreaKm\"].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max 16-Day Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.iloc[df16Day[\"AreaKm\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Annual Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_annual_range = np.mean(dfAnnual.AreaKm)\n",
    "mean_annual_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent square pixel cell dimension\n",
    "np.sqrt(mean_annual_range * 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min Annual Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.iloc[dfAnnual[\"AreaKm\"].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Annual Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.iloc[dfAnnual[\"AreaKm\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range sumarized by metaregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(x):\n",
    "    return np.subtract(*np.percentile(x, [75, 25]))\n",
    "\n",
    "def percentile25(x):\n",
    "    return np.percentile(x, 25)\n",
    "\n",
    "def percentile75(x):\n",
    "    return np.percentile(x, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.pivot_table(index=['MetaRegion'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.median, percentile25, percentile75, iqr], #np.mean, stats.sem,\n",
    "                            fill_value='NA').T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAnnual.pivot_table(index=['MetaRegion'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.median, percentile25, percentile75, iqr], #np.mean, stats.sem,\n",
    "                            fill_value='NA').T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range summarized by species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.pivot_table(index=['Species'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.median, percentile25, percentile75, iqr], #np.mean, stats.sem,\n",
    "                            fill_value='NA').T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.pivot_table(index=['Species'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.median, percentile25, percentile75, iqr], #np.mean, stats.sem,\n",
    "                            fill_value='NA').T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range sumarized by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.pivot_table(index=['Sex'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.median, percentile25, percentile75, iqr], #np.mean, stats.sem,\n",
    "                            fill_value='NA').T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.pivot_table(index=['Sex'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.median, percentile25, percentile75, iqr], #np.mean, stats.sem,\n",
    "                            fill_value='NA').T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETD-MCP-KDE Range Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 90th percentile ETD Data\n",
    "df16Day = pd.read_csv(filepath_or_buffer=etd16DayAnalysisTableFilt + '_new', header=0, parse_dates=True)\n",
    "dfAnnual = pd.read_csv(filepath_or_buffer=etdAnnualAnalysisTableFilt + '_new', header=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ETD ranges, MCP Ranges and KDE ranges\n",
    "df16Day_ETD = df16Day[['CalcID','MetaRegion','country', 'region', 'Area', 'IntersectPercent' ]]\n",
    "df16Day_ETD.set_index('CalcID', inplace=True, drop=True)\n",
    "df16Day_ETD.columns=['Biome','Country', 'Site', 'ETD', 'PAI']\n",
    "\n",
    "dfAnnual_ETD = dfAnnual[['CalcID','MetaRegion','country', 'region', 'Area', 'IntersectPercent']]\n",
    "dfAnnual_ETD.set_index('CalcID', inplace=True, drop=True)\n",
    "dfAnnual_ETD.columns=['Biome','Country', 'Site', 'ETD', 'PAI']\n",
    "\n",
    "df16Day_KDE = pd.read_csv(filepath_or_buffer=RangeFolder + '\\\\range_comparison\\\\KDE16DayPolygonGCS.csv',\n",
    "                          header=0)\n",
    "df16Day_KDE = df16Day_KDE[['CalcID','Area']]\n",
    "df16Day_KDE.set_index('CalcID', inplace=True, drop=True)\n",
    "df16Day_KDE.columns=['KDE']\n",
    "\n",
    "dfAnnual_KDE = pd.read_csv(filepath_or_buffer=RangeFolder + '\\\\range_comparison\\\\KDEAnnualPolygonGCS.csv',\n",
    "                           header=0)\n",
    "dfAnnual_KDE = dfAnnual_KDE[['CalcID','Area']]\n",
    "dfAnnual_KDE.set_index('CalcID', inplace=True, drop=True)\n",
    "dfAnnual_KDE.columns=['KDE']\n",
    "\n",
    "df16Day_MCP = pd.read_csv(filepath_or_buffer=RangeFolder + '\\\\range_comparison\\\\MCP16DayPolygonGCS.csv',\n",
    "                          header=0)\n",
    "df16Day_MCP = df16Day_MCP[['CalcID','MCPArea']]\n",
    "df16Day_MCP.set_index('CalcID', inplace=True, drop=True)\n",
    "df16Day_MCP.columns=['MCP']\n",
    "\n",
    "dfAnnual_MCP = pd.read_csv(filepath_or_buffer=RangeFolder + '\\\\range_comparison\\\\MCPAnnualPolygonGCS.csv',\n",
    "                           header=0)\n",
    "dfAnnual_MCP = dfAnnual_MCP[['CalcID','MCPArea']]\n",
    "dfAnnual_MCP.set_index('CalcID', inplace=True, drop=True)\n",
    "dfAnnual_MCP.columns=['MCP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Annual_ranges = dfAnnual_ETD.join(dfAnnual_KDE, how='inner').join(dfAnnual_MCP, how='inner')\n",
    "all_Annual_ranges['timespan'] = 'Annual'\n",
    "all_16Day_ranges = df16Day_ETD.join(df16Day_KDE, how='inner').join(df16Day_MCP, how='inner')\n",
    "all_16Day_ranges['timespan'] = '16Day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allranges=pd.concat([all_16Day_ranges, all_Annual_ranges])\n",
    "allranges['ETD'] = allranges['ETD']/1000000\n",
    "allranges['KDE'] = allranges['KDE']/1000000\n",
    "allranges['MCP'] = allranges['MCP']/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allranges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_summary_pivot = allranges.pivot_table(index=['Biome','Country','Site','timespan'],\n",
    "                            values=['KDE', 'MCP', 'ETD', 'PAI'],\n",
    "                            aggfunc=[max, np.median, np.mean, min],\n",
    "                            fill_value='NA').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_summary_pivot.to_csv(tablesFolder + 'range_size_pivot_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariate Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_summary(vals):\n",
    "    result = OrderedDict()\n",
    "    result['0%'] = np.min(vals)\n",
    "    result['5%'] = np.percentile(vals, 5.)\n",
    "    result['25%'] =  np.percentile(vals, 25.)\n",
    "    result['50%'] = np.percentile(vals, 50.)\n",
    "    result['75%'] = np.percentile(vals, 75.)\n",
    "    result['95%'] = np.percentile(vals, 95.)\n",
    "    result['100%'] = np.max(vals)\n",
    "    result['mean'] = np.mean(vals)\n",
    "    result['sd'] = np.std(vals)\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16-Day Range Covariate Summary\n",
    "df16Day[['AreaKm','ndviVal','treeVal','trmmVal','tempVal','waterVal','slopeVal','hfVal','IntersectPercent']] \\\n",
    " .apply(cov_summary).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.ndviVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.treeVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.trmmVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.tempVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.waterVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.slopeVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.hfVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df16Day.IntersectPercent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual Range & Covariate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.pivot_table(index=['MetaRegion'],\n",
    "                            values=['AreaKm'],\n",
    "                            aggfunc=[np.percentile(25),np.percentile(50),np.percentile(75)],\n",
    "                            fill_value='NA').T #.to_csv(tablesFolder + '\\\\Covariate_stats_by_metaregion_annual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual[['AreaKm','ndviVal','treeVal','trmmVal','tempVal','waterVal','slopeVal','hfVal','IntersectPercent']]. \\\n",
    "apply(func=cov_summary, axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD NDVI Values')\n",
    "sns.distplot(dfAnnual.ndviVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_NDVI_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Tree Cover Values')\n",
    "sns.distplot(dfAnnual.treeVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_Tree_Cover_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Rainfall Rate Values')\n",
    "sns.distplot(dfAnnual.trmmVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_Rainfall_Rate_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Land Surface Temperature Values')\n",
    "sns.distplot(dfAnnual.tempVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_LST_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Water Occurence Values')\n",
    "sns.distplot(dfAnnual.waterVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_WATER_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Slope Values')\n",
    "sns.distplot(dfAnnual.slopeVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_SLOPE_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Human Footprint Values')\n",
    "sns.distplot(dfAnnual.hfVal)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_HF_Dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,8)\n",
    "ax.set_title('Distribution of Annual ETD Protected Area Intersection Values')\n",
    "sns.distplot(dfAnnual.IntersectPercent)\n",
    "plt.savefig(figuresFolder + r'/distributions/Annual_PAI_Dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max HF Value - Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.iloc[dfAnnual[\"hfVal\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max HF Value 16-Day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16Day.iloc[df16Day[\"hfVal\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min HF Value - Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAnnual.iloc[dfAnnual[\"hfVal\"].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Water Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df16Day.iloc[df16Day[\"waterVal\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.iloc[dfAnnual[\"waterVal\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does Temperature variance change with temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does tempeprature variance increase with temperature?\n",
    "temp_variance = df16Day['tempVal'].groupby(pd.cut(df16Day.tempVal, np.arange(10, 52, 2))).agg(\n",
    "    {'mean': np.mean, 'var': np.var,})\n",
    "plt.figure()\n",
    "temp_variance.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join MCP, Path Metrics to Annual DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual ETD ranges Dataframe\n",
    "dfAnnual = pd.read_csv(filepath_or_buffer=etdAnnualAnalysisTableFilt,header=0, parse_dates=True)\n",
    "dfAnnual['ETD90Area'] = dfAnnual.AreaKm\n",
    "dfAnnual.index = dfAnnual.CalcID\n",
    "len(dfAnnual.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAnnualMCP100 = ConvertFeatureClassToPandasDataFrame(mcpRangesAnnual_Path + \"\\\\\" + 'MCPAnnualPolygonsGCS')\n",
    "dfAnnualMCP100 = dfAnnualMCP100.set_index('CalcID')\n",
    "dfAnnualMCP100=dfAnnualMCP100[dfAnnualMCP100[\"ChosenPercentile\"]==100][['MCPArea']]\n",
    "dfAnnualMCP100['MCP100Area'] = dfAnnualMCP100.MCPArea/1000000\n",
    "dfAnnualMCP100 = dfAnnualMCP100[['MCP100Area']]\n",
    "#dfAnnualMCP100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnualETD100 = ConvertFeatureClassToPandasDataFrame(etdRangesAnnual_Path + \"\\\\\" + 'ETDAnnualPolygonsGCS')\n",
    "dfAnnualETD100 = dfAnnualETD100.set_index('CalcID')\n",
    "dfAnnualETD100=dfAnnualETD100[dfAnnualETD100[\"DesiredPercentile\"]==1][['Area']]\n",
    "dfAnnualETD100['ETD100Area'] = dfAnnualETD100.Area/1000000\n",
    "dfAnnualETD100 = dfAnnualETD100[['ETD100Area']]\n",
    "#dfAnnualETD100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnualPaths = ConvertFeatureClassToPandasDataFrame(trajPathMetricsAnnual_Path + \"\\\\TrajPathMetrics\")\n",
    "dfAnnualPaths = dfAnnualPaths.set_index('CalcID')\n",
    "dfAnnualPaths = dfAnnualPaths[['DisplMtrs','MaxDisplMtrs','PathDistMtrs','Tortuosity','SpeedKmHrs']]\n",
    "dfAnnualPaths['PathDistKm'] = dfAnnualPaths['PathDistMtrs']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual = dfAnnual.join(dfAnnualETD100).join(dfAnnualMCP100).join(dfAnnualPaths)\n",
    "len(dfAnnual.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual.to_csv(path_or_buf=AnnualAnalysisTable, date_format='%Y-%m-%d %H:%M:%S',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dfAnnualETD100)\n",
    "del(dfAnnualMCP100)\n",
    "del(dfAnnualPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize range statistics by Meta-Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangeSizeSummary  =  dfAnnual.pivot_table(index=['MetaRegion'], values=['ETD90Area','ETD100Area','MCP100Area'],aggfunc=[min,np.mean,max])\n",
    "rangeSizeSummary.to_csv(tablesFolder + '\\\\RangeSizeSummary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print AnnualAnalysisTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare MCP-ETD-KDE Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnnual = pd.read_csv(AnnualAnalysisTable, parse_dates=True, header=0)\n",
    "len(dfAnnual.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform the ETD and MCP ranges\n",
    "dfAnnual['log_MCP100Area'] = np.log(dfAnnual.MCP100Area)\n",
    "dfAnnual['log_ETD100Area'] = np.log(dfAnnual.ETD100Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ETD/MCP ratio\n",
    "dfAnnual['MCP/ETD Ratio'] = dfAnnual['MCP100Area']/dfAnnual['ETD100Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('bmh'):\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 7))\n",
    "    g = sns.scatterplot(x='log_MCP100Area', y='log_ETD100Area', data=dfAnnual, hue='MetaRegion', size='MCP/ETD Ratio',\n",
    "                        sizes=(40, 600), ax=ax, alpha=0.8, legend='brief')\\n\n",
    "    ax.set_ylabel(\\\"Log - ETD Area (Sq.Km)\\\")\n",
    "    ax.set_xlabel(\\\"Log - MCP Area (Sq.Km)\\\")\n",
    "    ax.set_xlim(2,13)\n",
    "    ax.set_ylim(2,10)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "    print(type(handles[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('bmh'):\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 7))\n",
    "    g = sns.scatterplot(x='MCP100Area', y='ETD100Area', hue=\"MetaRegion\", data=dfAnnual, s=80, ax=ax, alpha=0.8)\n",
    "    ax.set_ylabel(\"ETD Area (Sq.Km)\")\n",
    "    ax.set_xlabel(\"MCP Area (Sq.Km)\")\n",
    "    ax.set_xlim(0,35000)\n",
    "    ax.set_ylim(0,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('bmh'):\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    title = fig.suptitle(\\\"ETD/MCP Area Ratio by MetaRegion\\\", fontsize=14)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    g = sns.FacetGrid(data=dfAnnual, hue='MetaRegion')\n",
    "    #g.map(sns.kdeplot, 'ETD_MCP_Ratio', kernel='tri', ax=ax)\n",
    "    g.map(sns.distplot,'MCP/ETD Ratio', kde=False, norm_hist=False, bins=30, ax=ax)\n",
    "    ax.legend(title='MetaRegion')\n",
    "    plt.close(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling ETD Range Size against covariates (R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-Day Range Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "### Initial R package import\n",
    "# install.packages(c('lattice','nlme','mgcv', 'MASS', 'MuMIn', 'ggplot2', 'gridExtra'))\n",
    "\n",
    "library(lattice)\n",
    "library(nlme)\n",
    "library(mgcv)\n",
    "library(MASS)\n",
    "library(MuMIn)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "theme_set(theme_gray(base_size=10))\n",
    "\n",
    "ctrl <- lmeControl(opt='optim')\n",
    "\n",
    "# Set working directory\n",
    "setwd('C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/Analysis Worksheet')\n",
    "\n",
    "#Read in the 16-Day PanAfEl16Day Data\n",
    "PanAfEl16Day<-read.csv(\"../../Tables/ETD_16Day_Analysis_Table_Filtered.csv_new\",header=T)\n",
    "nrow(PanAfEl16Day) #10322\n",
    "\n",
    "PanAfElAnnual<-read.csv(\"../../Tables/ETD_Annual_Analysis_Table_Filtered.csv\",header=T)\n",
    "nrow(PanAfElAnnual) #302\n",
    "\n",
    "###Calculate Days From Start Function Definition\n",
    "calcTemporalLag<-function(movDataID,inputDF)\n",
    "{\n",
    "    #Calculate the earliest date\n",
    "    minDate<-min(inputDF[inputDF$MovDataID == movDataID,\"StartDate2\"])\n",
    "    \n",
    "    #Calculate the difference in days for \n",
    "    inputDF[inputDF$MovDataID == movDataID,\"Days\"]<-difftime(inputDF[inputDF$MovDataID == movDataID,\"StartDate2\"],minDate,units=\"days\")\n",
    "    \n",
    "    #Calculate difference in years\n",
    "    inputDF[inputDF$MovDataID == movDataID,\"Years\"]<-inputDF[inputDF$MovDataID == movDataID,\"Days\"]/365.25\n",
    "    \n",
    "    return(inputDF)\n",
    "}\n",
    "\n",
    "#panel.cor for use with the pairs function\n",
    "panel.cor <- function(x, y, digits = 2, prefix = \"\", cex.cor, ...) \n",
    "{ \n",
    "  usr <- par(\"usr\"); on.exit(par(usr)) \n",
    "  par(usr = c(0, 1, 0, 1)) \n",
    "  r <- abs(cor(x, y)) \n",
    "  txt <- format(c(r, 0.123456789), digits = digits)[1] \n",
    "  txt <- paste0(prefix, txt) \n",
    "  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt) \n",
    "  text(0.5, 0.5, txt, cex = cex.cor * r) \n",
    "} \n",
    "\n",
    "panel.hist <- function(x, ...)\n",
    "{\n",
    "    usr <- par(\"usr\"); on.exit(par(usr))\n",
    "    par(usr = c(usr[1:2], 0, 1.5) )\n",
    "    h <- hist(x, plot = FALSE)\n",
    "    breaks <- h$breaks; nB <- length(breaks)\n",
    "    y <- h$counts; y <- y/max(y)\n",
    "    rect(breaks[-nB], 0, breaks[-1], y, col=\"cyan\", ...)\n",
    "}\n",
    "\n",
    "DiagnosticPlot<-function(theModel,yObserved,timeVals,modelLevel,outputFigureName)\n",
    "{\n",
    "    #theModel.resid.1<-resid(theModel,level=modelLevel, type=\"normalized\")\n",
    "    theModel.resid.1<-resid(theModel,level=modelLevel)\n",
    "    theModel.yhat.1<-fitted(theModel,level=modelLevel) #note there is a subtle diff between fitted and predicted commands\n",
    "    #Get the Model name\n",
    "    model.name<-deparse(substitute(theModel))\n",
    "\n",
    "    #Setup the png output file\n",
    "    #pdf(file=outputFigureName,width=8,height=5,pointsize=12,family=\"Times\") #paste(model.name,\"_DiagnosticPlot.pdf\")\n",
    "    png(file=outputFigureName,width=8,height=5,family=\"Times\", res=300, units = 'in')\n",
    "    par(mfrow=c(2,2),mai=c(0.6,0.6,0.6,0.6),cex=0.55)\n",
    "\n",
    "    #Note the order of these plots mimics the gam.fit function output\n",
    "    #Graph 1: QQ Plot of model residuals\n",
    "    qqnorm(theModel.resid.1,main=paste(\"Model\",model.name,\", Normality plot\",sep=\" \"))\n",
    "\n",
    "    #Graph 2: Linear Predictor (X) Vs Model Residuals (Y)\n",
    "    plot(theModel.yhat.1,theModel.resid.1, main=paste(\"Model\",model.name,\", Residual Plot\",sep=\" \"),\n",
    "      xlab=\"Predicted Value (log(Area Sq.Km))\", ylab=\"residual\")\n",
    "\n",
    "    #Graph 3: Histogram of model residuals\n",
    "    hist(theModel.resid.1, breaks=8 , density=10,col=\"green\", border=\"black\",xlab=paste(\"Model\",model.name,\"residuals (level 1)\",sep=\" \"),\n",
    "         main=paste(\"Model\",model.name,\", Error Distribution\",sep=\" \"))\n",
    "\n",
    "    #Graph 4: Observed values Vs Model Predicted values\n",
    "    plot(theModel.yhat.1,yObserved, main=paste(\"Model\",model.name,\", yhat vs log(Area Sq.Km)\",sep=\" \"),\n",
    "         xlab=\"Predicted Value\", ylab=paste(\"Observed Value\",model.name,sep=\" \"))\n",
    "    abline(0,1,col='red')\n",
    "\n",
    "    #Graph: Observed times values Vs Model residuals\n",
    "    #plot(timeVals,theModel.resid.1, main=paste(\"Model\",model.name,\", Residual vs Days\",sep=\" \"),xlab=\"Days\", ylab=\"residual\",pch=8)\n",
    "\n",
    "    par(mfrow=c(1,1),mai=c(1.0,1.0,1.0,1.0),cex=1.0)\n",
    "\n",
    "    dev.off()\n",
    "}\n",
    "\n",
    "#Function to retrieve legend from ggplot object\n",
    "get_legend<-function(myggplot){\n",
    "  tmp <- ggplot_gtable(ggplot_build(myggplot))\n",
    "  leg <- which(sapply(tmp$grobs, function(x) x$name) == \"guide-box\")\n",
    "  legend <- tmp$grobs[[leg]]\n",
    "  return(legend)\n",
    "}\n",
    "                      \n",
    "mround <- function(x,base){ \n",
    "        base*round(x/base) \n",
    "}\n",
    "\n",
    "get_CI <- function(y,pref=\"\") {\n",
    "    r1 <- t(apply(y,1,quantile,c(0.025,0.975)))\n",
    "    setNames(as.data.frame(r1),paste0(pref,c(\"lwr\",\"upr\")))\n",
    "}\n",
    "\n",
    "standardize<-function(x){\n",
    "    s.x <- scale(x)\n",
    "    return(s.x)\n",
    "}\n",
    "\n",
    "unstandardize<-function(s.x, x){\n",
    "    scl <- scale(x)\n",
    "    return(s.x * attr(scl, 'scaled:scale') + attr(scl, 'scaled:center'))\n",
    "}\n",
    "\n",
    "model_covars<-function(model){\n",
    "    # return the names of the model continuous covariates\n",
    "    covars <-as.character(attr(terms(model), \"term.labels\"))\n",
    "    covars <- na.omit(gsub(\"^\\\\w+:(\\\\w.*)\", NA, covars)) # regex match to any character sequence followed by a colon followed by character sequence\n",
    "    covars <- covars[! covars %in% names(lapply(model$contrasts, rownames))]\n",
    "    return(covars)\n",
    "}\n",
    "\n",
    "model_predict <- function(model, dataF, yaxis_spacing=25)\n",
    "{\n",
    "    minmax_y<-c(Inf,0) # create a dummy global y-axis range that will be adjusted depending on the data\n",
    "    xn = 100 # the number of x samples to make predictions at\n",
    "    bn = 10000 # the number of times to resample the model coefficients\n",
    "\n",
    "    # Pick n random draws from a nultivariate normal distribution defined by a mean vector (estimated model parameter values) \n",
    "    #and variance-covariance matrix (estimated parameter variances are on the diagonal). \n",
    "    # So we are effectively choosing bn realizations of possible model parameters based on their estimated mean and variance values\n",
    "    params_resamp <- mvrnorm(n=bn, mu = fixef(model), Sigma = vcov(model))\n",
    "\n",
    "    # Get the names of the model continuous covariates\n",
    "    covars <- model_covars(model)\n",
    "    print(covars)\n",
    "\n",
    "    plots <-vector(\"list\", length(covars)+1) # create an empty vector to hold plot objects as they are created\n",
    "    j=1\n",
    "\n",
    "    for(covar in covars){\n",
    "        print(covar)\n",
    "        # start a new panel in the prediction figure\n",
    "        covars_ <- covars[! covars %in% covar]  # vector of covariate names other than the current covariate\n",
    "\n",
    "        # Create a list of xn values ranging from the min to the max for the given covariate. \n",
    "        x_range <- setNames(list(seq(min(standardize(dataF[covar])), max(standardize(dataF[covar])), length.out=xn)), covar)\n",
    "\n",
    "        # Create a dataframe where each combination of factor is represented with the covariate ranging from it's min to max value \n",
    "        newdata <- expand.grid(c(lapply(model$contrasts, rownames), x_range))\n",
    "\n",
    "        # add the other covariates to the dataframe held at the 0 level (their mean if they are centered)\n",
    "        newdata[covars_]<-0 \n",
    "\n",
    "        # use the predictor variables (RHS) from the model formula and create the model matrix including all interactions, contrasts and dummy coding\n",
    "        model_matrix <- model.matrix(eval(model$call$fixed)[-2], data=newdata)\n",
    "\n",
    "        pred_func <- function(B){return(model_matrix %*% B)}\n",
    "\n",
    "        # predict mean response based on the REML value of regression coefficients\n",
    "        newdata[\"predict_y\"] = predict(model, newdata, level=0)\n",
    "\n",
    "        # use resampled regression coefficients to generate confidence intervals for the regression line\n",
    "        yvals <- apply(params_resamp, 1, pred_func)\n",
    "\n",
    "        c1 <- get_CI(yvals)\n",
    "\n",
    "        newdata[\"predict_y_upr\"] = c1[\"upr\"]\n",
    "        newdata[\"predict_y_lwr\"] = c1[\"lwr\"]\n",
    "        newdata[\"y\"] = as.numeric(unlist(exp(newdata[\"predict_y\"])))\n",
    "        newdata[\"y_upr\"] = as.numeric(unlist(exp(newdata[\"predict_y_upr\"])))\n",
    "        newdata[\"y_lwr\"] = as.numeric(unlist(exp(newdata[\"predict_y_lwr\"])))\n",
    "\n",
    "        # For plotting, need to transform the standardized x range back to the original scale\n",
    "        newdata$xvals <- as.numeric(unlist(unstandardize(newdata[covar], dataF[gsub('s.','',covar)]))) # assumes the dataframe uses the s. prefix for standardized variables\n",
    "\n",
    "        # adjust the y-axis\n",
    "        minmax_y[1] = min(newdata[\"y\"],newdata[\"y_lwr\"])\n",
    "        minmax_y[2] = max(newdata[\"y\"],newdata[\"y_upr\"])\n",
    "\n",
    "        print(minmax_y)\n",
    "        \n",
    "        newdata$interact <- interaction(newdata[names(model$contrasts)])\n",
    "\n",
    "        #newdata <- na.omit(newdata)\n",
    "        \n",
    "        p<-ggplot(data=newdata, aes(x=xvals, y=y, group=interact, colour=interact)) + geom_line(size=0.5) + \n",
    "        labs(x=gsub('s.','',covar), y=\"Range Area (Sq.Km)\", title=\"\") + theme(legend.title=element_blank()) +\n",
    "        geom_line(data=newdata, aes(x=xvals, y=y_upr, group=interact, colour=interact), size=0.5, linetype=\"dotted\") +\n",
    "        geom_line(data=newdata, aes(x=xvals, y=y_lwr, group=interact, colour=interact), size=0.5, linetype=\"dotted\")\n",
    "\n",
    "        #Use a common scale for the y-axis\n",
    "        plots[[length(covars)+1]] = get_legend(p)\n",
    "        plots[[j]] = p\n",
    "        j=j+1\n",
    "    }\n",
    "\n",
    "    #Use a common scale for the y-axis\n",
    "    minmax_y<-c(mround(minmax_y[1],5), mround(minmax_y[2],5))\n",
    "    breaks <- seq(minmax_y[1], minmax_y[2], by=yaxis_spacing)\n",
    "    for(i in 1:length(covars))\n",
    "    {\n",
    "        plots[[i]] = plots[[i]] + scale_y_continuous(limits=minmax_y, breaks=breaks, labels=breaks)\n",
    "        plots[[i]] = plots[[i]] + theme(legend.position = \"none\")\n",
    "    }\n",
    "    \n",
    "    return(plots)\n",
    "}\n",
    "\n",
    "# 16-Day data setup & filters\n",
    "\n",
    "#Convert the start/end time columns to datetime objects and append to the dataframe\n",
    "PanAfEl16Day$StartDate2<-as.POSIXct(PanAfEl16Day$StartDate, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "PanAfEl16Day$EndDate2<-as.POSIXct(PanAfEl16Day$EndDate, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#Calculate the temporal lag of each observation in both days and years\n",
    "for(s in unlist(list(unique(PanAfEl16Day$MovDataID))))\n",
    "{\n",
    "    PanAfEl16Day<-calcTemporalLag(s,PanAfEl16Day)\n",
    "}\n",
    "\n",
    "#Order the data alphabetically and temporally\n",
    "PanAfEl16Day<-PanAfEl16Day[order(PanAfEl16Day$MovDataID,PanAfEl16Day$Days),]\n",
    "\n",
    "# rename elephant to 'savannah elephant'\n",
    "PanAfEl16Day[which(PanAfEl16Day$Species==\"Elephant\"),]$Species <- \"Savannah Elephant\"\n",
    "\n",
    "#Calculate factors\n",
    "PanAfEl16Day$fSpecies<-factor(PanAfEl16Day$Species)\n",
    "PanAfEl16Day$fSex<-factor(PanAfEl16Day$Sex)\n",
    "PanAfEl16Day$fRegion<-factor(PanAfEl16Day$region)\n",
    "\n",
    "#Log transformations following Zuur et al.(2009) pgs. 129 & 131\n",
    "PanAfEl16Day$logArea<-log(PanAfEl16Day$AreaKm)\n",
    "\n",
    "\n",
    "### Standardize covariates and names\n",
    "#See http://warnercnr.colostate.edu/~gwhite/mark/markhelp/standardize_individual_covariates.htm\n",
    "\n",
    "#NDVI\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'ndviVal'] <- 'NDVI'\n",
    "PanAfEl16Day$s.NDVI<-standardize(PanAfEl16Day$NDVI)\n",
    "\n",
    "#NDWI\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'ndwiVal'] <- 'NDWI'\n",
    "PanAfEl16Day$s.NDWI<-standardize(PanAfEl16Day$NDWI)\n",
    "\n",
    "#EVI\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'eviVal'] <- 'EVI'\n",
    "PanAfEl16Day$s.EVI<-standardize(PanAfEl16Day$EVI)\n",
    "\n",
    "#Temperature\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'tempVal'] <- 'LST'\n",
    "PanAfEl16Day$s.LST<-standardize(PanAfEl16Day$LST)\n",
    "\n",
    "#TreeCover\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'treeVal'] <- 'TREE'\n",
    "PanAfEl16Day$s.TREE<-standardize(PanAfEl16Day$TREE)\n",
    "\n",
    "#Water\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'waterVal'] <- 'WATER'\n",
    "PanAfEl16Day$s.WATER<-standardize(PanAfEl16Day$WATER)\n",
    "\n",
    "#TRMM\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'trmmVal'] <- 'TRMM'\n",
    "PanAfEl16Day$s.TRMM<-standardize(PanAfEl16Day$TRMM)\n",
    "\n",
    "#HF\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'hfVal'] <- 'HFI'\n",
    "PanAfEl16Day$s.HFI<-standardize(PanAfEl16Day$HFI)\n",
    "\n",
    "#Slope\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'slopeVal'] <- 'SLOPE'\n",
    "PanAfEl16Day$s.SLOPE<-standardize(PanAfEl16Day$SLOPE)\n",
    "\n",
    "#PAIntersect\n",
    "names(PanAfEl16Day)[names(PanAfEl16Day) == 'IntersectPercent'] <- 'PAI'\n",
    "PanAfEl16Day$s.PAI<-standardize(PanAfEl16Day$PAI)\n",
    "\n",
    "#TGI\n",
    "PanAfEl16Day$s.TGI<-standardize(PanAfEl16Day$TGI)\n",
    "\n",
    "\n",
    "## Annual Data setup\n",
    "\n",
    "#Convert the start/end time columns to datetime objects and append to the dataframe\n",
    "PanAfElAnnual$StartDate2<-as.POSIXct(PanAfElAnnual$StartDate, format=\"%Y-%m-%d %H:%M\")\n",
    "PanAfElAnnual$EndDate2<-as.POSIXct(PanAfElAnnual$EndDate, format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "#Calculate the temporal lag of each observation in both days and years\n",
    "for(s in unlist(list(unique(PanAfElAnnual$MovDataID))))\n",
    "{\n",
    "    PanAfElAnnual<-calcTemporalLag(s,PanAfElAnnual)\n",
    "}\n",
    "\n",
    "#Order the data alphabetically and temporally\n",
    "PanAfElAnnual<-PanAfElAnnual[order(PanAfElAnnual$MovDataID,PanAfElAnnual$Days),]\n",
    "\n",
    "# rename elephant to 'savannah elephant'\n",
    "PanAfElAnnual[which(PanAfElAnnual$Species==\"Elephant\"),]$Species <- \"Savannah Elephant\"\n",
    "\n",
    "#Calculate factors\n",
    "PanAfElAnnual$fSpecies<-factor(PanAfElAnnual$Species)\n",
    "PanAfElAnnual$fSex<-factor(PanAfElAnnual$Sex)\n",
    "PanAfElAnnual$fRegion<-factor(PanAfElAnnual$region)\n",
    "\n",
    "#Create an interaction term between sex and MetaRegion\n",
    "PanAfElAnnual$SexRegion<-interaction(PanAfElAnnual$Sex,PanAfElAnnual$MetaRegion)\n",
    "\n",
    "#transformation following Zuur et al.(2009) pgs. 129 & 131\n",
    "PanAfElAnnual$logArea<-log(PanAfElAnnual$AreaKm)\n",
    "\n",
    "### Standardize covariates and names\n",
    "\n",
    "#NDVI\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'ndviVal'] <- 'NDVI'\n",
    "PanAfElAnnual$s.NDVI<-standardize(PanAfElAnnual$NDVI)\n",
    "\n",
    "#NDWI\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'ndwiVal'] <- 'NDWI'\n",
    "PanAfElAnnual$s.NDWI<-standardize(PanAfElAnnual$NDWI)\n",
    "\n",
    "#EVI\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'eviVal'] <- 'EVI'\n",
    "PanAfElAnnual$s.EVI<-standardize(PanAfElAnnual$EVI)\n",
    "\n",
    "#Temperature\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'tempVal'] <- 'LST'\n",
    "PanAfElAnnual$s.LST<-standardize(PanAfElAnnual$LST)\n",
    "\n",
    "#TreeCover\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'treeVal'] <- 'TREE'\n",
    "PanAfElAnnual$s.TREE<-standardize(PanAfElAnnual$TREE)\n",
    "\n",
    "#Water\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'waterVal'] <- 'WATER'\n",
    "PanAfElAnnual$s.WATER<-standardize(PanAfElAnnual$WATER)\n",
    "\n",
    "#TRMM\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'trmmVal'] <- 'TRMM'\n",
    "PanAfElAnnual$s.TRMM<-standardize(PanAfElAnnual$TRMM)\n",
    "\n",
    "#HF\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'hfVal'] <- 'HFI'\n",
    "PanAfElAnnual$s.HFI<-standardize(PanAfElAnnual$HFI)\n",
    "\n",
    "#Slope\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'slopeVal'] <- 'SLOPE'\n",
    "PanAfElAnnual$s.SLOPE<-standardize(PanAfElAnnual$SLOPE)\n",
    "\n",
    "#PAIntersect\n",
    "names(PanAfElAnnual)[names(PanAfElAnnual) == 'IntersectPercent'] <- 'PAI'\n",
    "PanAfElAnnual$s.PAI<-standardize(PanAfElAnnual$PAI)\n",
    "\n",
    "#TGI\n",
    "PanAfElAnnual$s.TGI<-standardize(PanAfElAnnual$TGI)\n",
    "\n",
    "\n",
    "## Covariate Plots\n",
    "\n",
    "#FIGURE: 16Day_Range_Covariate_PairPlot - Pair Plot of explanatory variables\n",
    "PanAfEl16DayXCov<-cbind(PanAfEl16Day$fSex,\n",
    "                        PanAfEl16Day$fSpecies,\n",
    "                        PanAfEl16Day$NDVI,\n",
    "                        PanAfEl16Day$NDWI,\n",
    "                        PanAfEl16Day$EVI,\n",
    "                        PanAfEl16Day$LST,\n",
    "                        PanAfEl16Day$TRMM,\n",
    "                        PanAfEl16Day$TREE,\n",
    "                        PanAfEl16Day$WATER,\n",
    "                        PanAfEl16Day$HFI,\n",
    "                        PanAfEl16Day$PAI,\n",
    "                        PanAfEl16Day$SLOPE)\n",
    "colnames(PanAfEl16DayXCov)<-c('SEX','SPECIES','NDVI','NDWI','EVI','LST','TRMM','TREE','WATER','SLOPE','HFI','PAI')\n",
    "\n",
    "# pdf(file=\"../../Figures/16Day_Range_Covariate_PairPlot.pdf\",width=6,height=6,pointsize=12,family=\"Times\")\n",
    "# pairs(PanAfEl16DayXCov,lower.panel=panel.smooth,upper.panel=panel.cor,diag.panel=panel.hist)\n",
    "# dev.off()\n",
    "\n",
    "png(file=\"../../Figures/16Day_Range_Covariate_PairPlot.png\",width=6,height=6,res=300, units='in')\n",
    "pairs(PanAfEl16DayXCov,lower.panel=panel.smooth,upper.panel=panel.cor,diag.panel=panel.hist)\n",
    "dev.off()\n",
    "\n",
    "#FIGURE: Annual_Range_Covariate_PairPlot.pdf - Pair Plot of explanatory variables\n",
    "PanAfElAnnualXCov<-cbind(PanAfElAnnual$fSex,\n",
    "                         PanAfElAnnual$fSpecies,\n",
    "                         PanAfElAnnual$NDVI,\n",
    "                         PanAfElAnnual$NDWI,\n",
    "                         PanAfElAnnual$EVI,\n",
    "                         PanAfElAnnual$LST,\n",
    "                         PanAfElAnnual$TRMM,\n",
    "                         PanAfElAnnual$TREE,\n",
    "                         PanAfElAnnual$WATER,\n",
    "                         PanAfElAnnual$HFI,\n",
    "                         PanAfElAnnual$PAI,\n",
    "                         PanAfElAnnual$SLOPE)\n",
    "colnames(PanAfElAnnualXCov)<-c('SEX','SPECIES','NDVI','NDWI','EVI','LST','TRMM','TREE','WATER','HFI','PAI','SLOPE')\n",
    "\n",
    "# pdf(file=\"../../Figures/Annual_Range_Covariate_PairPlot.pdf\",width=6,height=6,pointsize=12,family=\"Times\")\n",
    "# pairs(PanAfElAnnualXCov,lower.panel=panel.smooth,upper.panel=panel.cor,diag.panel=panel.hist)\n",
    "# dev.off()\n",
    "\n",
    "png(file=\"../../Figures/Annual_Range_Covariate_PairPlot.png\",width=6,height=6,res=300, units='in')\n",
    "pairs(PanAfElAnnualXCov,lower.panel=panel.smooth,upper.panel=panel.cor,diag.panel=panel.hist)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Modeling procedure based on 'Top-Down' described by West et al 2014 pg. 39.\n",
    "\n",
    "### STEP 1: Start with a well-specified mean structure\n",
    "\n",
    "# Colinearity between s.NDVI, s.NDWI, s.EVI so need to choose best model\n",
    "\n",
    "# s.NDVI model\n",
    "f_full_s.NDVI <- formula(logArea~1 + Sex + Species + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.NDVI<-lme(f_full_s.NDVI, random=~1|fRegion/MovDataID, data=PanAfEl16Day, method='ML',\n",
    "                   correlation=corCAR1(form=~Years|fRegion/MovDataID))\n",
    "\n",
    "# s.EVI model\n",
    "f_full_s.EVI <- formula(logArea~1 + Sex + Species + s.EVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.EVI<-lme(f_full_s.EVI, random=~1|fRegion/MovDataID, data=PanAfEl16Day, method='ML',\n",
    "                  correlation=corCAR1(form=~Years|fRegion/MovDataID))\n",
    "\n",
    "# s.NDWI model\n",
    "f_full_s.NDWI <- formula(logArea~1 + Sex + Species + s.NDWI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.NDWI<-lme(f_full_s.NDWI, random=~1|fRegion/MovDataID, data=PanAfEl16Day, method='ML',\n",
    "                   correlation=corCAR1(form=~Years|fRegion/MovDataID))\n",
    "  \n",
    "results<-model.sel(m_full_s.NDVI, m_full_s.EVI, m_full_s.NDWI)\n",
    "results # Model selection favours m_full_s.NDVI \n",
    "\n",
    "\"\"\"\n",
    "Model selection table \n",
    "              (Int)     s.HFI   s.NDV    s.PAI     s.SLO    s.LST      s.TRE\n",
    "m_full_s.NDVI 2.874 -0.03466 0.03173 -0.03997 -0.009853 0.017750 -0.0001751\n",
    "m_full_s.EVI  2.874 -0.03480         -0.03974 -0.009566 0.011340  0.0033610\n",
    "m_full_s.NDWI 2.871 -0.03543         -0.03965 -0.009214 0.009605  0.0048670\n",
    "                 s.TRM  s.WAT Sex Spc   s.EVI   s.NDW             family df\n",
    "m_full_s.NDVI 0.007597 0.1643   +   +                 gaussian(identity) 15\n",
    "m_full_s.EVI  0.007496 0.1644   +   + 0.01864         gaussian(identity) 15\n",
    "m_full_s.NDWI 0.007124 0.1644   +   +         0.01655 gaussian(identity) 15\n",
    "                 logLik    AICc delta weight\n",
    "m_full_s.NDVI -9371.128 18772.3  0.00  0.780\n",
    "m_full_s.EVI  -9372.983 18776.0  3.71  0.122\n",
    "m_full_s.NDWI -9373.203 18776.5  4.15  0.098\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\n",
    "\"\"\"\n",
    "\n",
    "### STEP 2: Select a structure for the random effects in the model\n",
    "## Select a structure for the random effects. Use REML here as we are testing random effects structures\n",
    "m_gls_0 <- gls(f_full_s.NDVI, data=PanAfEl16Day, method='REML', control=ctrl) # See pg. 93 of West et al. \n",
    "m_rand_1 <- lme(f_full_s.NDVI, random=~1|fRegion, data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "m_rand_2 <- lme(f_full_s.NDVI, random=~1|MovDataID, data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "m_rand_3 <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1), data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "results <-model.sel(m_gls_0, m_rand_1, m_rand_2, m_rand_3)\n",
    "results # Model selection favours m_rand_3\n",
    "'''\n",
    "Model selection table \n",
    "         (Int)     s.HFI   s.NDV    s.PAI     s.SLO     s.LST    s.TRE     s.TRM  s.WAT Sex Spc             family class  random df    logLik    AICc   delta weight\n",
    "m_rand_3 2.863 -0.04213 0.04205 -0.05319  0.011720 -0.016290 -0.02982 -0.009721 0.2062   +   + gaussian(identity)   lme f+M%i%f 14 -11051.22 22130.5    0.00      1\n",
    "m_rand_2 2.994 -0.04104 0.04146 -0.04725  0.005336 -0.014270 -0.02423 -0.009575 0.2032   +   + gaussian(identity)   lme       M 13 -11097.23 22220.5   90.00      0\n",
    "m_rand_1 2.852 -0.09973 0.05459 -0.04511 -0.027760  0.003221 -0.01828 -0.010930 0.1738   +   + gaussian(identity)   lme       f 13 -12026.21 24078.5 1947.96      0\n",
    "m_gls_0  3.023 -0.10940 0.04855  0.03920 -0.137800  0.055720  0.03570 -0.024610 0.1536   +   + gaussian(identity)   gls         12 -12610.32 25244.7 3114.18      0\n",
    "Models ranked by AICc(x) \n",
    "Random terms: \n",
    "f = ‘1 | fRegion’\n",
    "M%i%f = ‘1 | MovDataID %in% fRegion’\n",
    "M = ‘1 | MovDataID’\n",
    "'''\n",
    "\n",
    "## Select a correlation structure for the residuals\n",
    "m_rand_3_cor <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                    correlation=corCAR1(form=~Years), data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "results <-model.sel(m_rand_3, m_rand_3_cor)\n",
    "results # m_rand_3_cor is selected\n",
    "\n",
    "'''\n",
    "Model selection table \n",
    "             (Int)     s.HFI   s.NDV    s.PAI     s.SLO    s.LST      s.TRE     s.TRM  s.WAT Sex Spc             family     correlation df     logLik    AICc   delta\n",
    "m_rand_3_cor 2.873 -0.03458 0.03171 -0.04006 -0.009554  0.01762 -0.0006426  0.007578 0.1645   +   + gaussian(identity) corCAR1(~Years) 15  -9403.885 18837.8    0.00\n",
    "m_rand_3     2.863 -0.04213 0.04205 -0.05319  0.011720 -0.01629 -0.0298200 -0.009721 0.2062   +   + gaussian(identity)                 14 -11051.223 22130.5 3292.67\n",
    "             weight\n",
    "m_rand_3_cor      1\n",
    "m_rand_3          0\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\n",
    "'''\n",
    "\n",
    "## select a covariance structure for the residuals\n",
    "m_rand_3_cor_weights_none <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                 correlation=corCAR1(form=~Years),\n",
    "                                 data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "m_rand_3_cor_weights_region <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                   weights = varIdent(form = ~1 | fRegion),\n",
    "                                   correlation=corCAR1(form=~Years),\n",
    "                                   data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "# m_rand_3_cor_weights_sex is throwing an error - not sure why? \n",
    "# m_rand_3_cor_weights_sex <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "#                                 weights = varIdent(form = ~1 | fSex),\n",
    "#                                 correlation=corCAR1(form=~Years),\n",
    "#                                 data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "m_rand_3_cor_weights_species <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                    weights = varIdent(form = ~1 | fSpecies),\n",
    "                                    correlation=corCAR1(form=~Years),\n",
    "                                    data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "results <-model.sel(m_rand_3_cor_weights_none, m_rand_3_cor_weights_region,\n",
    "                     m_rand_3_cor_weights_species)\n",
    "results # Model m_rand_3_cor_weights_region is selected\n",
    "\n",
    "\"\"\"\n",
    "Model selection table \n",
    "                             (Int)     s.HFI   s.NDV    s.PAI     s.SLO   s.LST      s.TRE    s.TRM  s.WAT Sex Spc             family               weights df\n",
    "m_rand_3_cor_weights_region  2.869 -0.03217 0.03553 -0.03737 -0.011340 0.02116 -0.0015590 0.008956 0.1645   +   + gaussian(identity)  varIdent(~1|fRegion) 33\n",
    "m_rand_3_cor_weights_none    2.873 -0.03458 0.03171 -0.04006 -0.009554 0.01762 -0.0006426 0.007578 0.1645   +   + gaussian(identity)                       15\n",
    "m_rand_3_cor_weights_species 2.872 -0.03435 0.03191 -0.04041 -0.009375 0.01772 -0.0011060 0.007657 0.1653   +   + gaussian(identity) varIdent(~1|fSpecies) 16\n",
    "                                logLik    AICc  delta weight\n",
    "m_rand_3_cor_weights_region  -9332.027 18730.3   0.00      1\n",
    "m_rand_3_cor_weights_none    -9403.885 18837.8 107.54      0\n",
    "m_rand_3_cor_weights_species -9403.332 18838.7 108.44      0\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Step 4: competing model selection\n",
    "\n",
    "f_sex <- formula(logArea~1 + Sex)\n",
    "m_sex <- lme(f_sex, random=list(fRegion=~1, MovDataID=~1),\n",
    "              weights = varIdent(form = ~1 | fRegion),\n",
    "              correlation=corCAR1(form=~Years),\n",
    "              data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_species <- formula(logArea~1 + Species)\n",
    "m_species <- lme(f_species, random=list(fRegion=~1, MovDataID=~1),\n",
    "              weights = varIdent(form = ~1 | fRegion),\n",
    "              correlation=corCAR1(form=~Years),\n",
    "              data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "\n",
    "f_indv <- formula(logArea~1 + Sex + Species)\n",
    "m_indv <- lme(f_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "              weights = varIdent(form = ~1 | fRegion),\n",
    "              correlation=corCAR1(form=~Years),\n",
    "              data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_anthro <- formula(logArea~1 + s.HFI + s.PAI)\n",
    "m_anthro <-lme(f_anthro, random=list(fRegion=~1, MovDataID=~1),\n",
    "               weights = varIdent(form = ~1 | fRegion),\n",
    "               correlation=corCAR1(form=~Years),\n",
    "               data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_anthro_indv <-formula(logArea~1 + Sex + Species + s.HFI + s.PAI)\n",
    "m_anthro_indv<-lme(f_anthro_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "                   weights = varIdent(form = ~1 | fRegion),\n",
    "                   correlation=corCAR1(form=~Years),\n",
    "                   data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic <- formula(logArea~1 + s.NDVI + s.TREE)\n",
    "m_biotic<-lme(f_biotic, random=list(fRegion=~1, MovDataID=~1),\n",
    "              weights = varIdent(form = ~1 | fRegion),\n",
    "              correlation=corCAR1(form=~Years),\n",
    "              data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_indv <- formula(logArea~1 + Sex + Species + s.NDVI + s.TREE)\n",
    "m_biotic_indv<-lme(f_biotic_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "                   weights = varIdent(form = ~1 | fRegion),\n",
    "                   correlation=corCAR1(form=~Years),\n",
    "                   data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_abiotic <- formula(logArea~1 + s.TRMM + s.WATER + s.LST + s.SLOPE)\n",
    "m_abiotic<-lme(f_abiotic, random=list(fRegion=~1, MovDataID=~1),\n",
    "               weights = varIdent(form = ~1 | fRegion),\n",
    "               correlation=corCAR1(form=~Years),\n",
    "               data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_abiotic_indv <- formula(logArea~1 + Sex + Species + s.TRMM + s.WATER + s.LST + s.SLOPE)\n",
    "m_abiotic_indv<-lme(f_abiotic_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "                    weights = varIdent(form = ~1 | fRegion),\n",
    "                    correlation=corCAR1(form=~Years),\n",
    "                    data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_abiotic <-formula(logArea~1 + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_biotic_abiotic<-lme(f_biotic_abiotic, random=list(fRegion=~1, MovDataID=~1),\n",
    "                      weights = varIdent(form = ~1 | fRegion),\n",
    "                      correlation=corCAR1(form=~Years),\n",
    "                      data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_abiotic_indv <-formula(logArea~1 + Sex + Species + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_biotic_abiotic_indv<-lme(f_biotic_abiotic_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_abiotic_sex <-formula(logArea~1 + Sex + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_biotic_abiotic_sex<-lme(f_biotic_abiotic_sex, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_abiotic_species <-formula(logArea~1 + Species + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_biotic_abiotic_species<-lme(f_biotic_abiotic_species, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_abiotic_anthro_species <-formula(logArea~1 + Species + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE +\n",
    "                                          s.HFI + s.PAI)\n",
    "m_biotic_abiotic_anthro_species<-lme(f_biotic_abiotic_anthro_species, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_biotic_abiotic_anthro_sex <-formula(logArea~1 + Sex + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE +\n",
    "                                          s.HFI + s.PAI)\n",
    "m_biotic_abiotic_anthro_sex<-lme(f_biotic_abiotic_anthro_sex, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "f_full <- formula(logArea~1 + Sex + Species + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full<-lme(f_full, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fRegion), \n",
    "            correlation=corCAR1(form=~Years), \n",
    "            data=PanAfEl16Day, method='ML', control=ctrl)\n",
    "\n",
    "mod_sel<-model.sel(\n",
    "    m_sex,\n",
    "    m_species,\n",
    "    m_indv,\n",
    "    m_anthro,\n",
    "    m_anthro_indv,\n",
    "    m_biotic,\n",
    "    m_biotic_indv,\n",
    "    m_abiotic,\n",
    "    m_abiotic_indv,\n",
    "    m_biotic_abiotic,\n",
    "    m_biotic_abiotic_indv,\n",
    "    m_biotic_abiotic_sex,\n",
    "    m_biotic_abiotic_species,\n",
    "    m_biotic_abiotic_anthro_species,\n",
    "    m_biotic_abiotic_anthro_sex,\n",
    "    m_full)\n",
    "\n",
    "mod_sel # Model m_biotic_abiotic_anthro_species is selected\n",
    "\n",
    "\n",
    "'''\n",
    "                                (Int) Sex Spc    s.HFI    s.PAI   s.NDV\n",
    "m_biotic_abiotic_anthro_species 2.868       + -0.03227 -0.03727 0.03556\n",
    "m_full                          2.870   +   + -0.03226 -0.03729 0.03556\n",
    "m_biotic_abiotic_anthro_sex     2.749   +     -0.03184 -0.03781 0.03563\n",
    "m_biotic_abiotic_species        2.888       +                   0.03461\n",
    "m_biotic_abiotic                2.773                           0.03468\n",
    "m_biotic_abiotic_indv           2.890   +   +                   0.03461\n",
    "m_biotic_abiotic_sex            2.771   +                       0.03468\n",
    "m_abiotic                       2.776                                  \n",
    "m_abiotic_indv                  2.889   +   +                          \n",
    "m_anthro                        2.748         -0.02964 -0.04536        \n",
    "m_anthro_indv                   2.825   +   + -0.03020 -0.04495        \n",
    "m_biotic                        2.773                           0.02363\n",
    "m_species                       2.842       +                          \n",
    "m_sex                           2.772   +                              \n",
    "m_biotic_indv                   2.844   +   +                   0.02347\n",
    "m_indv                          2.844   +   +                          \n",
    "                                     s.TRE     s.LST    s.SLO    s.TRM  s.WAT\n",
    "m_biotic_abiotic_anthro_species -0.0011350 0.0212800 -0.01160 0.008969 0.1643\n",
    "m_full                          -0.0011200 0.0212900 -0.01161 0.008971 0.1644\n",
    "m_biotic_abiotic_anthro_sex     -0.0044190 0.0210700 -0.01050 0.008825 0.1643\n",
    "m_biotic_abiotic_species         0.0007031 0.0176500 -0.01621 0.010200 0.1644\n",
    "m_biotic_abiotic                -0.0026040 0.0174700 -0.01509 0.010050 0.1644\n",
    "m_biotic_abiotic_indv            0.0007117 0.0176500 -0.01622 0.010210 0.1644\n",
    "m_biotic_abiotic_sex            -0.0025980 0.0174700 -0.01508 0.010050 0.1644\n",
    "m_abiotic                                  0.0006545 -0.01411 0.010060 0.1645\n",
    "m_abiotic_indv                             0.0003190 -0.01445 0.010190 0.1647\n",
    "m_anthro                                                                     \n",
    "m_anthro_indv                                                                \n",
    "m_biotic                        -0.0078180                                   \n",
    "m_species                                                                    \n",
    "m_sex                                                                        \n",
    "m_biotic_indv                   -0.0057050                                   \n",
    "m_indv                                                                       \n",
    "                                            family df    logLik    AICc  delta\n",
    "m_biotic_abiotic_anthro_species gaussian(identity) 32 -9299.146 18662.5   0.00\n",
    "m_full                          gaussian(identity) 33 -9299.144 18664.5   2.01\n",
    "m_biotic_abiotic_anthro_sex     gaussian(identity) 32 -9300.393 18665.0   2.49\n",
    "m_biotic_abiotic_species        gaussian(identity) 30 -9308.374 18676.9  14.43\n",
    "m_biotic_abiotic                gaussian(identity) 29 -9309.605 18677.4  14.88\n",
    "m_biotic_abiotic_indv           gaussian(identity) 31 -9308.373 18678.9  16.44\n",
    "m_biotic_abiotic_sex            gaussian(identity) 30 -9309.603 18679.4  16.89\n",
    "m_abiotic                       gaussian(identity) 27 -9313.533 18681.2  18.72\n",
    "m_abiotic_indv                  gaussian(identity) 29 -9312.395 18683.0  20.46\n",
    "m_anthro                        gaussian(identity) 25 -9492.417 19035.0 372.46\n",
    "m_anthro_indv                   gaussian(identity) 27 -9491.764 19037.7 375.18\n",
    "m_biotic                        gaussian(identity) 25 -9500.547 19051.2 388.72\n",
    "m_species                       gaussian(identity) 24 -9502.226 19052.6 390.07\n",
    "m_sex                           gaussian(identity) 24 -9502.822 19053.8 391.26\n",
    "m_biotic_indv                   gaussian(identity) 27 -9499.946 19054.0 391.54\n",
    "m_indv                          gaussian(identity) 25 -9502.224 19054.6 392.08\n",
    "                                weight\n",
    "m_biotic_abiotic_anthro_species  0.604\n",
    "m_full                           0.221\n",
    "m_biotic_abiotic_anthro_sex      0.174\n",
    "m_biotic_abiotic_species         0.000\n",
    "m_biotic_abiotic                 0.000\n",
    "m_biotic_abiotic_indv            0.000\n",
    "m_biotic_abiotic_sex             0.000\n",
    "m_abiotic                        0.000\n",
    "m_abiotic_indv                   0.000\n",
    "m_anthro                         0.000\n",
    "m_anthro_indv                    0.000\n",
    "m_biotic                         0.000\n",
    "m_species                        0.000\n",
    "m_sex                            0.000\n",
    "m_biotic_indv                    0.000\n",
    "m_indv                           0.000\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Refit with REML\n",
    "f_biotic_abiotic_anthro_species <-formula(logArea~1 + fSpecies + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE +\n",
    "                                          s.HFI + s.PAI)\n",
    "m_biotic_abiotic_anthro_species<-lme(f_biotic_abiotic_anthro_species, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "\n",
    "## Create a diagnostic plot of the conditional model\n",
    "DiagnosticPlot(m_biotic_abiotic_anthro_species, PanAfEl16Day$logArea, PanAfEl16Day$Years,0,\n",
    "               \"../../Figures/16Day_Model_Diagnostics.png\")\n",
    "\n",
    "## Export model summary table\n",
    "summary(m_biotic_abiotic_anthro_species)\n",
    "write.csv(summary(m_biotic_abiotic_anthro_species)$tTable, file = \"../../Tables/tTable_16Day.csv\")\n",
    "\n",
    "'''\n",
    "Linear mixed-effects model fit by REML\n",
    " Data: PanAfEl16Day \n",
    "       AIC      BIC    logLik\n",
    "  18724.44 18956.15 -9330.221\n",
    "\n",
    "Random effects:\n",
    " Formula: ~1 | fRegion\n",
    "        (Intercept)\n",
    "StdDev:   0.5021581\n",
    "\n",
    " Formula: ~1 | MovDataID %in% fRegion\n",
    "        (Intercept)  Residual\n",
    "StdDev:   0.3719902 0.7194702\n",
    "\n",
    "Correlation Structure: Continuous AR(1)\n",
    " Formula: ~Years | fRegion/MovDataID \n",
    " Parameter estimate(s):\n",
    "         Phi \n",
    "1.713915e-06 \n",
    "Variance function:\n",
    " Structure: Different standard deviations per stratum\n",
    " Formula: ~1 | fRegion \n",
    " Parameter estimates:\n",
    "          APNR    Asante Sana   Chyulu Hills          Coast  Dzanga-Sangha         Gourma         Ivindo         Kruger       Laikipia           Lewa         Loango \n",
    "     1.0000000      0.8740580      0.9970340      0.8992496      1.1897613      1.2298731      1.0857930      1.0744045      0.9680075      0.7872732      0.7663703 \n",
    "          Lope       Marsabit     Masai Mara    Mount Kenya       Northern Nouabale-Ndoki         Odzala        Samburu \n",
    "     1.3644978      0.9221654      0.9504148      0.9538801      1.0587643      0.9561824      1.1282823      0.8919332 \n",
    "Fixed effects: list(f_biotic_abiotic_anthro_species) \n",
    "                             Value  Std.Error    DF   t-value p-value\n",
    "(Intercept)              2.8659540 0.14744735 10082 19.437135  0.0000\n",
    "fSpeciesForest Elephant -0.4563130 0.29248493    17 -1.560125  0.1372\n",
    "s.NDVI                   0.0355306 0.01254201 10082  2.832925  0.0046\n",
    "s.TREE                  -0.0015587 0.01289879 10082 -0.120841  0.9038\n",
    "s.WATER                  0.1645239 0.00836087 10082 19.677831  0.0000\n",
    "s.TRMM                   0.0089561 0.00607060 10082  1.475327  0.1402\n",
    "s.LST                    0.0211690 0.01155403 10082  1.832174  0.0670\n",
    "s.SLOPE                 -0.0113435 0.01033266 10082 -1.097828  0.2723\n",
    "s.HFI                   -0.0322079 0.01013008 10082 -3.179438  0.0015\n",
    "s.PAI                   -0.0373571 0.01250028 10082 -2.988501  0.0028\n",
    " Correlation: \n",
    "                        (Intr) fSpcFE s.NDVI s.TREE s.WATE s.TRMM s.LST  s.SLOP s.HFI \n",
    "fSpeciesForest Elephant -0.502                                                        \n",
    "s.NDVI                   0.002  0.005                                                 \n",
    "s.TREE                  -0.011 -0.139 -0.228                                          \n",
    "s.WATER                  0.015 -0.027 -0.011 -0.001                                   \n",
    "s.TRMM                   0.002 -0.014 -0.008  0.035 -0.009                            \n",
    "s.LST                   -0.003 -0.003  0.494  0.081 -0.030  0.135                     \n",
    "s.SLOPE                 -0.026  0.046  0.004 -0.247 -0.001 -0.025  0.093              \n",
    "s.HFI                    0.010  0.025 -0.006  0.053 -0.020  0.054 -0.079 -0.128       \n",
    "s.PAI                    0.038 -0.022 -0.015 -0.010  0.027  0.015 -0.021 -0.017  0.008\n",
    "\n",
    "Standardized Within-Group Residuals:\n",
    "        Min          Q1         Med          Q3         Max \n",
    "-6.38653643 -0.63041311 -0.01589249  0.61323266  4.13886384 \n",
    "\n",
    "Number of Observations: 10319\n",
    "Number of Groups: \n",
    "               fRegion MovDataID %in% fRegion \n",
    "                    19                    229 \n",
    "\n",
    "'''\n",
    "\n",
    "## Amount of explained variation (marginal, conditional)\n",
    "r.squaredGLMM(m_biotic_abiotic_anthro_species)\n",
    "\n",
    "'''\n",
    "            R2m       R2c\n",
    "[1,] 0.03232268 0.4484489\n",
    "Warning message:\n",
    "'r.squaredGLMM' now calculates a revised statistic. See the help page. \n",
    "'''\n",
    "\n",
    "myplots <- model_predict(m_biotic_abiotic_anthro_species, dataF=PanAfEl16Day,  yaxis_spacing=5)\n",
    "png(\"../../Figures/16Day_Model_Predictions_new2.png\", width = 10, height=7, res=300, units='in')\n",
    "do.call(\"grid.arrange\", c(myplots))\n",
    "dev.off()\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "### Get model summaries for other top models\n",
    "\n",
    "f_full <- formula(logArea~1 + Sex + Species + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full<-lme(f_full, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fRegion), \n",
    "            correlation=corCAR1(form=~Years), \n",
    "            data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "summary(m_full)\n",
    "\n",
    "'''\n",
    "Linear mixed-effects model fit by REML\n",
    " Data: PanAfEl16Day \n",
    "       AIC   BIC    logLik\n",
    "  18730.05 18969 -9332.027\n",
    "\n",
    "Random effects:\n",
    " Formula: ~1 | fRegion\n",
    "        (Intercept)\n",
    "StdDev:   0.5023377\n",
    "\n",
    " Formula: ~1 | MovDataID %in% fRegion\n",
    "        (Intercept)  Residual\n",
    "StdDev:   0.3733526 0.7194919\n",
    "\n",
    "Correlation Structure: Continuous AR(1)\n",
    " Formula: ~Years | fRegion/MovDataID \n",
    " Parameter estimate(s):\n",
    "         Phi \n",
    "1.715189e-06 \n",
    "Variance function:\n",
    " Structure: Different standard deviations per stratum\n",
    " Formula: ~1 | fRegion \n",
    " Parameter estimates:\n",
    "          APNR    Asante Sana   Chyulu Hills          Coast  Dzanga-Sangha         Gourma         Ivindo         Kruger       Laikipia           Lewa         Loango \n",
    "     1.0000000      0.8741717      0.9968651      0.9002842      1.1893316      1.2298694      1.0855842      1.0744214      0.9679591      0.7872526      0.7668488 \n",
    "          Lope       Marsabit     Masai Mara    Mount Kenya       Northern Nouabale-Ndoki         Odzala        Samburu \n",
    "     1.3643014      0.9221668      0.9504260      0.9538899      1.0587472      0.9564952      1.1264494      0.8918706 \n",
    "Fixed effects: list(f_full) \n",
    "                            Value  Std.Error    DF   t-value p-value\n",
    "(Intercept)             2.8685519 0.15182856 10082 18.893362  0.0000\n",
    "SexMale                -0.0046579 0.06549945   209 -0.071114  0.9434\n",
    "SpeciesForest Elephant -0.4581508 0.29358969    17 -1.560514  0.1371\n",
    "s.NDVI                  0.0355283 0.01254263 10082  2.832605  0.0046\n",
    "s.TREE                 -0.0015590 0.01290230 10082 -0.120834  0.9038\n",
    "s.WATER                 0.1645347 0.00836136 10082 19.677989  0.0000\n",
    "s.TRMM                  0.0089557 0.00607054 10082  1.475271  0.1402\n",
    "s.LST                   0.0211604 0.01155447 10082  1.831360  0.0671\n",
    "s.SLOPE                -0.0113359 0.01033635 10082 -1.096707  0.2728\n",
    "s.HFI                  -0.0321719 0.01013138 10082 -3.175474  0.0015\n",
    "s.PAI                  -0.0373745 0.01250463 10082 -2.988851  0.0028\n",
    " Correlation: \n",
    "                       (Intr) SexMal SpcsFE s.NDVI s.TREE s.WATE s.TRMM s.LST  s.SLOP s.HFI \n",
    "SexMale                -0.236                                                               \n",
    "SpeciesForest Elephant -0.505  0.079                                                        \n",
    "s.NDVI                  0.003 -0.004  0.005                                                 \n",
    "s.TREE                 -0.007 -0.016 -0.140 -0.228                                          \n",
    "s.WATER                 0.013  0.005 -0.026 -0.011 -0.001                                   \n",
    "s.TRMM                  0.002  0.000 -0.014 -0.008  0.035 -0.009                            \n",
    "s.LST                  -0.004  0.001 -0.003  0.494  0.081 -0.030  0.135                     \n",
    "s.SLOPE                -0.031  0.024  0.047  0.004 -0.247 -0.001 -0.025  0.093              \n",
    "s.HFI                   0.013 -0.011  0.024 -0.006  0.053 -0.020  0.054 -0.079 -0.129       \n",
    "s.PAI                   0.032  0.021 -0.020 -0.015 -0.010  0.027  0.015 -0.021 -0.017  0.008\n",
    "\n",
    "Standardized Within-Group Residuals:\n",
    "       Min         Q1        Med         Q3        Max \n",
    "-6.3865848 -0.6303741 -0.0160936  0.6127268  4.1392038 \n",
    "\n",
    "Number of Observations: 10319\n",
    "Number of Groups: \n",
    "               fRegion MovDataID %in% fRegion \n",
    "                    19                    229 \n",
    "'''\n",
    "\n",
    "f_biotic_abiotic_anthro_sex <-formula(logArea~1 + Sex + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE +\n",
    "                                          s.HFI + s.PAI)\n",
    "m_biotic_abiotic_anthro_sex<-lme(f_biotic_abiotic_anthro_sex, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fRegion),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfEl16Day, method='REML', control=ctrl)\n",
    "summary(m_biotic_abiotic_anthro_sex)\n",
    "\n",
    "'''\n",
    "Linear mixed-effects model fit by REML\n",
    " Data: PanAfEl16Day \n",
    "       AIC      BIC   logLik\n",
    "  18729.72 18961.43 -9332.86\n",
    "\n",
    "Random effects:\n",
    " Formula: ~1 | fRegion\n",
    "        (Intercept)\n",
    "StdDev:   0.5405705\n",
    "\n",
    " Formula: ~1 | MovDataID %in% fRegion\n",
    "        (Intercept)  Residual\n",
    "StdDev:   0.3727141 0.7194288\n",
    "\n",
    "Correlation Structure: Continuous AR(1)\n",
    " Formula: ~Years | fRegion/MovDataID \n",
    " Parameter estimate(s):\n",
    "         Phi \n",
    "1.702386e-06 \n",
    "Variance function:\n",
    " Structure: Different standard deviations per stratum\n",
    " Formula: ~1 | fRegion \n",
    " Parameter estimates:\n",
    "          APNR    Asante Sana   Chyulu Hills          Coast  Dzanga-Sangha         Gourma         Ivindo         Kruger       Laikipia           Lewa         Loango \n",
    "     1.0000000      0.8730920      0.9968061      0.9026468      1.1804900      1.2296425      1.0859116      1.0743150      0.9678144      0.7874126      0.7675898 \n",
    "          Lope       Marsabit     Masai Mara    Mount Kenya       Northern Nouabale-Ndoki         Odzala        Samburu \n",
    "     1.3650608      0.9221470      0.9504166      0.9536880      1.0588261      0.9363118      1.1372466      0.8917197 \n",
    "Fixed effects: list(f_biotic_abiotic_anthro_sex) \n",
    "                 Value  Std.Error    DF   t-value p-value\n",
    "(Intercept)  2.7487732 0.13899283 10082 19.776368  0.0000\n",
    "SexMale      0.0022779 0.06528185   209  0.034893  0.9722\n",
    "s.NDVI       0.0355984 0.01254264 10082  2.838190  0.0045\n",
    "s.TREE      -0.0044692 0.01279344 10082 -0.349337  0.7268\n",
    "s.WATER      0.1644114 0.00836273 10082 19.660010  0.0000\n",
    "s.TRMM       0.0088256 0.00606999 10082  1.453974  0.1460\n",
    "s.LST        0.0210052 0.01155444 10082  1.817936  0.0691\n",
    "s.SLOPE     -0.0104193 0.01032795 10082 -1.008848  0.3131\n",
    "s.HFI       -0.0318067 0.01012892 10082 -3.140190  0.0017\n",
    "s.PAI       -0.0378116 0.01250631 10082 -3.023399  0.0025\n",
    " Correlation: \n",
    "        (Intr) SexMal s.NDVI s.TREE s.WATE s.TRMM s.LST  s.SLOP s.HFI \n",
    "SexMale -0.214                                                        \n",
    "s.NDVI   0.005 -0.004                                                 \n",
    "s.TREE  -0.088 -0.006 -0.229                                          \n",
    "s.WATER  0.000  0.007 -0.011 -0.005                                   \n",
    "s.TRMM  -0.006  0.001 -0.008  0.033 -0.009                            \n",
    "s.LST   -0.006  0.001  0.494  0.082 -0.030  0.135                     \n",
    "s.SLOPE -0.007  0.020  0.004 -0.243  0.001 -0.024  0.093              \n",
    "s.HFI    0.028 -0.013 -0.006  0.057 -0.020  0.055 -0.079 -0.130       \n",
    "s.PAI    0.024  0.022 -0.015 -0.013  0.026  0.015 -0.021 -0.016  0.008\n",
    "\n",
    "Standardized Within-Group Residuals:\n",
    "        Min          Q1         Med          Q3         Max \n",
    "-6.38940786 -0.62963180 -0.01551716  0.61270068  4.13889000 \n",
    "\n",
    "Number of Observations: 10319\n",
    "Number of Groups: \n",
    "               fRegion MovDataID %in% fRegion \n",
    "                    19                    229 \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual Range Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Modeling procedure based on 'Top-Down' described by West et al 2014 pg. 39.\n",
    "\n",
    "## STEP 1: Start with a well-specified mean structure\n",
    "\n",
    "# Colinearity between s.NDVI, s.EVI, s.LST, s.TREE  so need to choose best full model\n",
    "\n",
    "# s.NDVI model\n",
    "f_full_s.NDVI <- formula(logArea~1 + fSex + fSpecies + s.NDVI + s.NDWI + s.WATER + s.TRMM + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.NDVI<-lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                   data=PanAfElAnnual, method='REML', control=ctrl)\n",
    "\n",
    "# s.EVI model\n",
    "f_full_s.EVI <- formula(logArea~1 + fSex + fSpecies + s.EVI + s.NDWI + s.WATER + s.TRMM + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.EVI<-lme(f_full_s.EVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                  data=PanAfElAnnual, method='REML', control=ctrl)\n",
    "\n",
    "# s.TEMP model\n",
    "f_full_s.LST <- formula(logArea~1 + fSex + fSpecies + s.NDWI + s.WATER + s.TRMM + s.LST + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.LST<-lme(f_full_s.LST, random=list(fRegion=~1, MovDataID=~1),\n",
    "                   data=PanAfElAnnual, method='REML', control=ctrl)\n",
    "\n",
    "# s.TREE model\n",
    "f_full_s.TREE <- formula(logArea~1 + fSex + fSpecies + s.NDWI + s.WATER + s.TRMM + s.TREE + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full_s.TREE<-lme(f_full_s.TREE, random=list(fRegion=~1, MovDataID=~1),\n",
    "                   data=PanAfElAnnual, method='REML', control=ctrl)\n",
    "\n",
    "results<-model.sel(m_full_s.NDVI, m_full_s.EVI, m_full_s.LST, m_full_s.TREE)\n",
    "results # Model selection favours s.NDVI. Adopt s.NDVI full model.\n",
    "\n",
    "'''\n",
    "Model selection table \n",
    "              (Int) fSx fSp   s.HFI  s.NDV    s.NDW   s.PAI     s.SLO   s.TRM     s.WAT   s.EVI    s.LST    s.TRE             family df   logLik  AICc delta weight\n",
    "m_full_s.NDVI 5.086   +   + -0.1135 0.0744 -0.09653 -0.1186 -0.024610 0.06013 -0.000722                           gaussian(identity) 13 -200.600 428.5  0.00  0.309\n",
    "m_full_s.EVI  5.084   +   + -0.1167        -0.09693 -0.1201 -0.020130 0.06096  0.002744 0.06571                   gaussian(identity) 13 -200.683 428.6  0.17  0.285\n",
    "m_full_s.TREE 5.053   +   + -0.1216        -0.03498 -0.1140 -0.002573 0.07316  0.003065                  -0.04456 gaussian(identity) 13 -200.970 429.2  0.74  0.214\n",
    "m_full_s.LST  5.072   +   + -0.1134        -0.06636 -0.1163 -0.025670 0.06578 -0.003081         -0.03686          gaussian(identity) 13 -201.075 429.4  0.95  0.192\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\n",
    "'''\n",
    "\n",
    "## Select a structure for the random effects\n",
    "m_gls_0 <- gls(f_full_s.NDVI, data=PanAfElAnnual, method='REML') # See pg. 93 of West et al. \n",
    "m_rand_1 <- lme(f_full_s.NDVI, random=~1|fRegion, data=PanAfElAnnual, method='REML')\n",
    "m_rand_2 <- lme(f_full_s.NDVI, random=~1|MovDataID, data=PanAfElAnnual, method='REML')\n",
    "m_rand_3 <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1), data=PanAfElAnnual, method='REML')\n",
    "results <-model.sel(m_gls_0, m_rand_1, m_rand_2, m_rand_3)\n",
    "results # Model selection favours m_rand_3\n",
    "'''\n",
    "Model selection table \n",
    "         (Int) fSx fSp   s.HFI    s.NDV    s.NDW      s.PAI     s.SLO    s.TRM      s.WAT             family class  random df   logLik  AICc  delta weight\n",
    "m_rand_3 5.086   +   + -0.1135  0.07440 -0.09653 -0.1186000 -0.024610  0.06013 -0.0007221 gaussian(identity)   lme f+M%i%f 13 -200.600 428.5   0.00      1\n",
    "m_rand_2 5.180   +   + -0.1232 -0.01306  0.06472  0.0008957 -0.174100 -0.05536 -0.0044040 gaussian(identity)   lme       M 12 -221.056 467.2  38.73      0\n",
    "m_rand_1 5.004   +   + -0.2255  0.02670 -0.10570 -0.1104000  0.006952  0.10790 -0.0576000 gaussian(identity)   lme       f 12 -251.398 527.9  99.41      0\n",
    "m_gls_0  5.131   +   + -0.2167 -0.01360  0.05267 -0.0252100 -0.209600 -0.12630 -0.0252100 gaussian(identity)   gls         11 -280.939 584.8 156.32      0\n",
    "Models ranked by AICc(x) \n",
    "Random terms: \n",
    "f = ‘1 | fRegion’\n",
    "M%i%f = ‘1 | MovDataID %in% fRegion’\n",
    "M = ‘1 | MovDataID’\n",
    "'''\n",
    "\n",
    "## Select a correlation structure for the residuals\n",
    "m_rand_3_cor <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                    correlation=corCAR1(form=~Years),\n",
    "                    data=PanAfElAnnual, method='REML', control=ctrl)\n",
    "results <-model.sel(m_rand_3, m_rand_3_cor)\n",
    "results #  m_rand_3_cor is selected\n",
    "\n",
    "\"\"\"\n",
    "Model selection table \n",
    "             (Int) fSx fSp   s.HFI   s.NDV    s.NDW   s.PAI    s.SLO   s.TRM      s.WAT             family     correlation control df   logLik  AICc delta weight\n",
    "m_rand_3_cor 5.080   +   + -0.1044 0.05507 -0.11060 -0.1200 -0.01411 0.07673 -0.0196800 gaussian(identity) corCAR1(~Years)    ctrl 14 -196.781 423.0  0.00  0.938\n",
    "m_rand_3     5.086   +   + -0.1135 0.07440 -0.09653 -0.1186 -0.02461 0.06013 -0.0007221 gaussian(identity)                         13 -200.600 428.5  5.44  0.062\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\n",
    "\"\"\"\n",
    "\n",
    "## select a covariance structure for the residuals\n",
    "m_rand_3_cor_weights_none <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                 correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "m_rand_3_cor_weights_region <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                   weights = varIdent(form = ~1 | fRegion),\n",
    "                                   correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "m_rand_3_cor_weights_sex <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                weights = varIdent(form = ~1 | fSex),\n",
    "                                correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "m_rand_3_cor_weights_species <- lme(f_full_s.NDVI, random=list(fRegion=~1, MovDataID=~1),\n",
    "                                    weights = varIdent(form = ~1 | fSpecies),\n",
    "                                    correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "results <-model.sel(m_rand_3_cor_weights_none, m_rand_3_cor_weights_region,\n",
    "                    m_rand_3_cor_weights_sex, m_rand_3_cor_weights_species)\n",
    "\n",
    "results # Model m_rand_3_cor_weights_sex is selected\n",
    "\"\"\"\n",
    "Model selection table \n",
    "                             (Int) fSx fSp    s.HFI   s.NDV    s.NDW   s.PAI    s.SLO   s.TRM      s.WAT             family               weights df   logLik  AICc\n",
    "m_rand_3_cor_weights_sex     5.084   +   + -0.11200 0.06426 -0.10390 -0.1496 -0.01049 0.06562  8.144e-05 gaussian(identity)     varIdent(~1|fSex) 15 -191.040 413.8\n",
    "m_rand_3_cor_weights_region  5.064   +   + -0.02701 0.12690 -0.10700 -0.2191 -0.02329 0.05889  9.337e-03 gaussian(identity)  varIdent(~1|fRegion) 27 -178.647 416.8\n",
    "m_rand_3_cor_weights_species 5.063   +   + -0.10740 0.03474 -0.09841 -0.1367 -0.01214 0.07841 -1.567e-02 gaussian(identity) varIdent(~1|fSpecies) 15 -194.229 420.1\n",
    "m_rand_3_cor_weights_none    5.080   +   + -0.10440 0.05507 -0.11060 -0.1200 -0.01411 0.07673 -1.968e-02 gaussian(identity)                       14 -196.781 423.0\n",
    "                             delta weight\n",
    "m_rand_3_cor_weights_sex      0.00  0.789\n",
    "m_rand_3_cor_weights_region   3.05  0.171\n",
    "m_rand_3_cor_weights_species  6.38  0.033\n",
    "m_rand_3_cor_weights_none     9.27  0.008\n",
    "Models ranked by AICc(x) \n",
    "Random terms (all models): \n",
    "‘1 | fRegion’, ‘1 | MovDataID %in% fRegion’\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Choose fixed effects with and without sex species interactions\n",
    "f_indv <- formula(logArea~1 + fSex + fSpecies)\n",
    "m_indv<-lme(f_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_anthro <- formula(logArea~1 + s.HFI + s.PAI)\n",
    "m_anthro <-lme(f_anthro, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_anthro_indv <-formula(logArea~1 + fSex + fSpecies + s.HFI + s.PAI)\n",
    "m_anthro_indv<-lme(f_anthro_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic <- formula(logArea~1 + s.NDVI + s.NDWI)\n",
    "m_biotic<-lme(f_biotic, random=list(fRegion=~1, MovDataID=~1), \n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_indv <- formula(logArea~1 + fSex + fSpecies + s.NDVI + s.NDWI)\n",
    "m_biotic_indv<-lme(f_biotic_indv, random=list(fRegion=~1, MovDataID=~1), \n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_abiotic <- formula(logArea~1 + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_abiotic<-lme(f_abiotic, random=list(fRegion=~1, MovDataID=~1), \n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_abiotic_indv <- formula(logArea~1 + fSex + fSpecies + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_abiotic_indv<-lme(f_abiotic_indv, random=list(fRegion=~1, MovDataID=~1), \n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_abiotic <-formula(logArea~1 + s.NDVI + s.NDWI + s.WATER + s.TRMM + s.SLOPE)\n",
    "m_biotic_abiotic<-lme(f_biotic_abiotic, random=list(fRegion=~1, MovDataID=~1), \n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_abiotic_indv <-formula(logArea~1 + fSex + fSpecies + s.NDVI + s.NDWI + s.WATER + s.TRMM + s.SLOPE)\n",
    "m_biotic_abiotic_indv<-lme(f_biotic_abiotic_indv, random=list(fRegion=~1, MovDataID=~1), \n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_abiotic_sex <-formula(logArea~1 + fSex + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_biotic_abiotic_sex<-lme(f_biotic_abiotic_sex, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fSex),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_abiotic_species <-formula(logArea~1 + fSpecies + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE)\n",
    "m_biotic_abiotic_species<-lme(f_biotic_abiotic_species, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fSex),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_abiotic_anthro_species <-formula(logArea~1 + fSpecies + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE +\n",
    "                                          s.HFI + s.PAI)\n",
    "m_biotic_abiotic_anthro_species<-lme(f_biotic_abiotic_anthro_species, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fSex),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_biotic_abiotic_anthro_sex <-formula(logArea~1 + fSex + s.NDVI + s.TREE + s.WATER + s.TRMM + s.LST + s.SLOPE +\n",
    "                                          s.HFI + s.PAI)\n",
    "m_biotic_abiotic_anthro_sex<-lme(f_biotic_abiotic_anthro_sex, random=list(fRegion=~1, MovDataID=~1),\n",
    "                           weights = varIdent(form = ~1 | fSex),\n",
    "                           correlation=corCAR1(form=~Years),\n",
    "                           data=PanAfElAnnual, method='ML')\n",
    "\n",
    "f_full <- formula(logArea~1 + fSex + fSpecies + s.NDVI + s.NDWI + s.WATER + s.TRMM + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full<-lme(f_full, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='ML')\n",
    "\n",
    "mod_sel<-model.sel(m_indv, m_anthro, m_anthro_indv, m_biotic, m_biotic_indv, m_abiotic,\n",
    "                   m_abiotic_indv, m_biotic_abiotic, m_biotic_abiotic_indv,\n",
    "                   m_biotic_abiotic_sex,\n",
    "                   m_biotic_abiotic_species,\n",
    "                   m_biotic_abiotic_anthro_species,\n",
    "                   m_biotic_abiotic_anthro_sex,\n",
    "                   m_full)\n",
    "\n",
    "mod_sel # m_anthro_indv is selected\n",
    "'''\n",
    "Model selection table \n",
    "                                (Int) fSx fSp    s.HFI   s.PAI    s.NDV    s.NDW     s.LST    s.SLO    s.TRM      s.WAT    s.TRE             family df   logLik\n",
    "m_anthro_indv                   5.036   +   + -0.11960 -0.1362                                                                   gaussian(identity) 10 -176.532\n",
    "m_anthro                        4.841         -0.09608 -0.1479                                                                   gaussian(identity)  8 -180.566\n",
    "m_full                          5.087   +   + -0.11360 -0.1477  0.06165 -0.09935           -0.01324 0.063380  0.0008561          gaussian(identity) 15 -174.390\n",
    "m_biotic_abiotic_anthro_species 5.106       + -0.12480 -0.1368  0.04569           0.004346 -0.02013 0.037720  0.0092450 -0.05528 gaussian(identity) 15 -175.804\n",
    "m_indv                          5.075   +   +                                                                                    gaussian(identity)  8 -183.733\n",
    "m_biotic_indv                   5.124   +   +                   0.07672 -0.10140                                                 gaussian(identity) 10 -182.646\n",
    "m_biotic_abiotic_anthro_sex     4.840   +     -0.10830 -0.1452  0.03994          -0.005716  0.00565 0.028080 -0.0183100 -0.10420 gaussian(identity) 15 -177.787\n",
    "m_biotic                        4.971                           0.07156 -0.11720                                                 gaussian(identity)  8 -185.397\n",
    "m_biotic_abiotic_indv           5.150   +   +                   0.06682 -0.12050           -0.04325 0.048270 -0.0086260          gaussian(identity) 13 -181.388\n",
    "m_abiotic_indv                  5.107   +   +                                    -0.023830 -0.06399 0.019800 -0.0011180          gaussian(identity) 12 -182.719\n",
    "m_biotic_abiotic                4.989                           0.06093 -0.14070           -0.03440 0.043360 -0.0373900          gaussian(identity) 11 -184.080\n",
    "m_biotic_abiotic_species        5.150       +                  -0.01963          -0.061050 -0.05984 0.018840 -0.0071780 -0.05438 gaussian(identity) 13 -182.640\n",
    "m_abiotic                       4.924                                            -0.015050 -0.05670 0.006071 -0.0304700          gaussian(identity) 10 -186.067\n",
    "m_biotic_abiotic_sex            4.949   +                      -0.01681          -0.063350 -0.03462 0.009664 -0.0288700 -0.10330 gaussian(identity) 13 -184.055\n",
    "                                 AICc delta weight\n",
    "m_anthro_indv                   373.8  0.00  0.829\n",
    "m_anthro                        377.6  3.80  0.124\n",
    "m_full                          380.5  6.64  0.030\n",
    "m_biotic_abiotic_anthro_species 383.3  9.47  0.007\n",
    "m_indv                          384.0 10.14  0.005\n",
    "m_biotic_indv                   386.0 12.23  0.002\n",
    "m_biotic_abiotic_anthro_sex     387.3 13.43  0.001\n",
    "m_biotic                        387.3 13.47  0.001\n",
    "m_biotic_abiotic_indv           390.0 16.22  0.000\n",
    "m_abiotic_indv                  390.5 16.70  0.000\n",
    "m_biotic_abiotic                391.1 17.25  0.000\n",
    "m_biotic_abiotic_species        392.5 18.73  0.000\n",
    "m_abiotic                       392.9 19.07  0.000\n",
    "m_biotic_abiotic_sex            395.4 21.55  0.000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## Refit with REML\n",
    "f_anthro_indv <-formula(logArea~1 + fSex + fSpecies + s.HFI + s.PAI)\n",
    "m_anthro_indv <-lme(f_anthro_indv, random=list(fRegion=~1, MovDataID=~1),\n",
    "                         weights = varIdent(form = ~1 | fSex),\n",
    "                         correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "\n",
    "## Create a diagnostic plot of the population model\n",
    "DiagnosticPlot(m_anthro_indv, PanAfElAnnual$logArea,PanAfElAnnual$Years,0, \"../../Figures/Annual_Model_Diagnostics.png\")\n",
    "\n",
    "## Export model summary table\n",
    "summary(m_anthro_indv)\n",
    "write.csv(summary(m_anthro_indv)$tTable, file = \"../../Tables/tTable_Annual.csv\")\n",
    "\n",
    "'''\n",
    "Linear mixed-effects model fit by REML\n",
    " Data: PanAfElAnnual \n",
    "       AIC      BIC    logLik\n",
    "  385.7162 422.6535 -182.8581\n",
    "\n",
    "Random effects:\n",
    " Formula: ~1 | fRegion\n",
    "        (Intercept)\n",
    "StdDev:    0.653593\n",
    "\n",
    " Formula: ~1 | MovDataID %in% fRegion\n",
    "        (Intercept)  Residual\n",
    "StdDev:   0.4121123 0.3866323\n",
    "\n",
    "Correlation Structure: Continuous AR(1)\n",
    " Formula: ~Years | fRegion/MovDataID \n",
    " Parameter estimate(s):\n",
    "      Phi \n",
    "0.3504766 \n",
    "Variance function:\n",
    " Structure: Different standard deviations per stratum\n",
    " Formula: ~1 | fSex \n",
    " Parameter estimates:\n",
    "     Male    Female \n",
    "1.0000000 0.6727519 \n",
    "Fixed effects: list(f_anthro_indv) \n",
    "                            Value Std.Error  DF   t-value p-value\n",
    "(Intercept)              5.032861 0.2103820 171 23.922487  0.0000\n",
    "fSexMale                 0.115915 0.0960162 114  1.207241  0.2298\n",
    "fSpeciesForest Elephant -1.299569 0.4914727  12 -2.644235  0.0214\n",
    "s.HFI                   -0.117579 0.0440601 171 -2.668598  0.0084\n",
    "s.PAI                   -0.138168 0.0527692 171 -2.618353  0.0096\n",
    " Correlation: \n",
    "                        (Intr) fSexMl fSpcFE s.HFI \n",
    "fSexMale                -0.211                     \n",
    "fSpeciesForest Elephant -0.418  0.040              \n",
    "s.HFI                    0.011 -0.041  0.182       \n",
    "s.PAI                    0.053 -0.008 -0.056 -0.104\n",
    "\n",
    "Standardized Within-Group Residuals:\n",
    "         Min           Q1          Med           Q3          Max \n",
    "-3.324456320 -0.553053626 -0.005699585  0.553857061  2.542789138 \n",
    "\n",
    "Number of Observations: 302\n",
    "Number of Groups: \n",
    "               fRegion MovDataID %in% fRegion \n",
    "                    14                    129 \n",
    "\n",
    "'''\n",
    "\n",
    "## Amount of explained variation (marginal, conditional)\n",
    "r.squaredGLMM(m_anthro_indv)\n",
    "\n",
    "'''\n",
    "            R2m       R2c\n",
    "[1,] 0.06799563 0.8133699\n",
    "Warning message:\n",
    "'r.squaredGLMM' now calculates a revised statistic. See the help page. \n",
    "\n",
    "'''\n",
    "\n",
    "## Generate prediction plots\n",
    "myplots <- model_predict(m_anthro_indv, dataF=PanAfElAnnual, yaxis_spacing=50)\n",
    "\n",
    "png(\"../../Figures/Annual_Model_Predictions_new2.png\", width = 7, height = 10, res=300, units='in')\n",
    "do.call(\"grid.arrange\", c(myplots))\n",
    "dev.off()\n",
    "\n",
    "# Get parameter values of other top two models------------------------------\n",
    "\n",
    "f_anthro <- formula(logArea~1 + s.HFI + s.PAI)\n",
    "m_anthro <-lme(f_anthro, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "summary(m_anthro)\n",
    "\n",
    "'''\n",
    "Linear mixed-effects model fit by REML\n",
    " Data: PanAfElAnnual \n",
    "       AIC      BIC    logLik\n",
    "  386.7548 416.3584 -185.3774\n",
    "\n",
    "Random effects:\n",
    " Formula: ~1 | fRegion\n",
    "        (Intercept)\n",
    "StdDev:   0.8227071\n",
    "\n",
    " Formula: ~1 | MovDataID %in% fRegion\n",
    "        (Intercept)  Residual\n",
    "StdDev:   0.4149744 0.3887844\n",
    "\n",
    "Correlation Structure: Continuous AR(1)\n",
    " Formula: ~Years | fRegion/MovDataID \n",
    " Parameter estimate(s):\n",
    "      Phi \n",
    "0.3613904 \n",
    "Variance function:\n",
    " Structure: Different standard deviations per stratum\n",
    " Formula: ~1 | fSex \n",
    " Parameter estimates:\n",
    "     Male    Female \n",
    "1.0000000 0.6676575 \n",
    "Fixed effects: list(f_anthro) \n",
    "                Value  Std.Error  DF   t-value p-value\n",
    "(Intercept)  4.839022 0.23004432 171 21.035172  0.0000\n",
    "s.HFI       -0.096757 0.04364868 171 -2.216725  0.0280\n",
    "s.PAI       -0.147901 0.05324849 171 -2.777570  0.0061\n",
    " Correlation: \n",
    "      (Intr) s.HFI \n",
    "s.HFI  0.077       \n",
    "s.PAI  0.026 -0.098\n",
    "\n",
    "Standardized Within-Group Residuals:\n",
    "         Min           Q1          Med           Q3          Max \n",
    "-3.301044073 -0.526112712  0.003942935  0.545517391  2.586206558 \n",
    "\n",
    "Number of Observations: 302\n",
    "Number of Groups: \n",
    "               fRegion MovDataID %in% fRegion \n",
    "                    14                    129 \n",
    "'''\n",
    "\n",
    "f_full <- formula(logArea~1 + fSex + fSpecies + s.NDVI + s.NDWI + s.WATER + s.TRMM + s.SLOPE + s.HFI + s.PAI)\n",
    "m_full<-lme(f_full, random=list(fRegion=~1, MovDataID=~1),\n",
    "            weights = varIdent(form = ~1 | fSex), correlation=corCAR1(form=~Years), data=PanAfElAnnual, method='REML')\n",
    "summary(m_full)\n",
    "\n",
    "'''\n",
    "Linear mixed-effects model fit by REML\n",
    " Data: PanAfElAnnual \n",
    "       AIC      BIC    logLik\n",
    "  412.0806 467.2319 -191.0403\n",
    "\n",
    "Random effects:\n",
    " Formula: ~1 | fRegion\n",
    "        (Intercept)\n",
    "StdDev:   0.7134374\n",
    "\n",
    " Formula: ~1 | MovDataID %in% fRegion\n",
    "        (Intercept)  Residual\n",
    "StdDev:   0.4041972 0.3904173\n",
    "\n",
    "Correlation Structure: Continuous AR(1)\n",
    " Formula: ~Years | fRegion/MovDataID \n",
    " Parameter estimate(s):\n",
    "      Phi \n",
    "0.3754444 \n",
    "Variance function:\n",
    " Structure: Different standard deviations per stratum\n",
    " Formula: ~1 | fSex \n",
    " Parameter estimates:\n",
    "     Male    Female \n",
    "1.0000000 0.6816292 \n",
    "Fixed effects: list(f_full) \n",
    "                              Value Std.Error  DF   t-value p-value\n",
    "(Intercept)                3.663349 0.5299611 166  6.912486  0.0000\n",
    "fSexMale                   0.104592 0.0972465 114  1.075532  0.2844\n",
    "fSpeciesSavannah Elephant  1.420974 0.5772171  12  2.461768  0.0299\n",
    "s.NDVI                     0.064265 0.0743832 166  0.863970  0.3889\n",
    "s.NDWI                    -0.103932 0.0708637 166 -1.466646  0.1444\n",
    "s.WATER                    0.000081 0.0452679 166  0.001799  0.9986\n",
    "s.TRMM                     0.065623 0.0432805 166  1.516234  0.1314\n",
    "s.SLOPE                   -0.010493 0.0515302 166 -0.203623  0.8389\n",
    "s.HFI                     -0.111966 0.0462861 166 -2.418993  0.0166\n",
    "s.PAI                     -0.149623 0.0536288 166 -2.789970  0.0059\n",
    " Correlation: \n",
    "                          (Intr) fSexMl fSpcSE s.NDVI s.NDWI s.WATE s.TRMM s.SLOP s.HFI \n",
    "fSexMale                  -0.073                                                        \n",
    "fSpeciesSavannah Elephant -0.916 -0.019                                                 \n",
    "s.NDVI                    -0.031 -0.109  0.076                                          \n",
    "s.NDWI                    -0.166  0.095  0.099 -0.577                                   \n",
    "s.WATER                   -0.326  0.042  0.322 -0.068  0.142                            \n",
    "s.TRMM                    -0.110  0.055  0.102 -0.340 -0.196  0.135                     \n",
    "s.SLOPE                    0.028  0.157 -0.070 -0.303 -0.050  0.013  0.251              \n",
    "s.HFI                      0.268 -0.084 -0.228  0.091 -0.204 -0.115 -0.072 -0.103       \n",
    "s.PAI                     -0.041 -0.016  0.059 -0.051  0.081  0.075 -0.069 -0.095 -0.096\n",
    "\n",
    "Standardized Within-Group Residuals:\n",
    "        Min          Q1         Med          Q3         Max \n",
    "-3.25677074 -0.52655069 -0.02966863  0.52370330  2.53946504 \n",
    "\n",
    "Number of Observations: 302\n",
    "Number of Groups: \n",
    "               fRegion MovDataID %in% fRegion \n",
    "                    14                    129 \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habitat Suitability Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// --- This code block is javascript and is meant to be run within the Google Earth Engine Code Editor --- //\n",
    "\n",
    "//-----------------------GLOBAL VARS------------------------------------------\n",
    "\n",
    "//IUCN AfESG distribution polygons coming straight from http://www.elephantdatabase.org/\n",
    "//var afelsg_range = ee.FeatureCollection('ft:133dT3pqdvGT-BmrZWW3ub5Q2Zr4qAWyWhkB7RZk');\n",
    "var aed_poly = ee.FeatureCollection(\"users/walljcg/AED_Range_2016_Known_Possible_Revised3\"); //cleaned\n",
    "var wdpa_poly = ee.FeatureCollection('users/walljcg/wdpa_2013');\n",
    "var africa_poly = ee.FeatureCollection(\"users/walljcg/ContinentalAfrica\");\n",
    "\n",
    "var pix_16day = 1000;\n",
    "var analysis_scale = 1000;\n",
    "\n",
    "// Visualizations\n",
    "var vis_dispersal = {\n",
    "  palette: ['006400']\n",
    "};\n",
    "var vis_aed = {\n",
    "  palette:['e0115f']\n",
    "};\n",
    "\n",
    "// NDVI\n",
    "var ndvi = ee.ImageCollection('MODIS/MCD43A4_NDVI').select('NDVI')\n",
    "    .reduce(ee.Reducer.mean());\n",
    "\n",
    "// TREE\n",
    "var tree = ee.ImageCollection('MODIS/006/MOD44B').select('Percent_Tree_Cover')\n",
    "    .reduce(ee.Reducer.mean());\n",
    "\n",
    "// WATER\n",
    "var water = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('occurrence').unmask(0);\n",
    "\n",
    "// TRMM\n",
    "var trmm = ee.ImageCollection('TRMM/3B42').select(['precipitation']) \n",
    "    .reduce(ee.Reducer.mean()); // should this be the sum?\n",
    "\n",
    "// LST\n",
    "var calibrateLST = function(image){\n",
    "        return image.multiply(ee.Image(0.02)).subtract(ee.Image(273.15));\n",
    "}\n",
    "var lst = ee.ImageCollection('MODIS/MOD11A2').select('LST_Day_1km').map(calibrateLST)\n",
    "    .reduce(ee.Reducer.mean());\n",
    "\n",
    "// SLOPE\n",
    "var slope = ee.Algorithms.Terrain(ee.Image('USGS/SRTMGL1_003')).select(['slope']);\n",
    "\n",
    "// HF\n",
    "var hf = ee.Image('users/walljcg/hf_wcs_2009');\n",
    "var hf_mask = hf.gte(0.0); // Mask out values less than zero\n",
    "hf = hf.updateMask(hf_mask);\n",
    "\n",
    "// PAI\n",
    "var pai = ee.Image(1.0).clip(wdpa_poly).unmask();\n",
    "\n",
    "//-----------------------HSM----------------------------------------------------\n",
    "\n",
    "var reduce_square = function(img, cell_dim){\n",
    "  return img.reduceNeighborhood(ee.Reducer.mean(), ee.Kernel.square(cell_dim,'meters'));\n",
    "}\n",
    "\n",
    "// HSM - using 16-Day min/max vals\n",
    "var hsm_dispersal = ee.Image(1.0)\n",
    ".updateMask(reduce_square(ndvi, pix_16day).gte(0.032054).and(reduce_square(ndvi, pix_16day).lte(0.855016))) // min\n",
    ".updateMask(reduce_square(tree, pix_16day).gte(0.088542).and(reduce_square(tree, pix_16day).lte(78.655488))) // min\n",
    ".updateMask(reduce_square(lst, pix_16day).gte(11.814545).and(reduce_square(lst, pix_16day).lte(51.620476))) // min-max\n",
    ".updateMask(reduce_square(water, pix_16day).lte(85.000000)) // min-max\n",
    ".updateMask(reduce_square(slope, pix_16day).lte(12.000000)) // max\n",
    ".updateMask(reduce_square(hf, pix_16day).lte(31.511565)); // min-max\n",
    "var hsm_dispersal_africa = hsm_dispersal.clip(africa_poly.geometry());\n",
    "var hsm_dispersal_poly = hsm_dispersal_africa.reduceToVectors({geometryType: 'polygon', scale: analysis_scale});\n",
    "\n",
    "//----------------------------------MAP------------------------------------------------\n",
    "\n",
    "Map.centerObject(hsm_dispersal_africa);\n",
    "Map.addLayer(hsm_dispersal_africa, vis_dispersal, 'HSM_Dispersal');\n",
    "Map.addLayer(aed_poly, vis_aed, 'AED Range');\n",
    "\n",
    "//----------------------------------EXPORT------------------------------------------------\n",
    "\n",
    "// Export.image.toDrive({image:hsm_dispersal_africa,\n",
    "//     description: \"PanAfEl_HSM_Dispersal\",\n",
    "//     folder: \"PanAfEl\",\n",
    "//     region: africa_poly.geometry(), //hsm_dispersal_poly,\n",
    "//     crs: \"EPSG:4326\",\n",
    "//     scale: analysis_scale});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert GCS to Albers to get aereal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.CreateFileGDB_management ('Y:/Research_PanAfEl/Analyses/HSM', 'HSM')\n",
    "arcpy.env.workspace ='C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM' + '/HSM.gdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set arcpy to overwrite output\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "if arcpy.CheckExtension(\"Spatial\") == \"Available\":\n",
    "        arcpy.CheckOutExtension(\"Spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continental Africa (without Madagascar) - Albers Equal Area Conic\n",
    "arcpy.Project_management(in_dataset=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Spatial Data/SpatialData.gdb/ContinentalAfrica\",\n",
    "                         out_dataset=\"ContinentalAfrica_albers\",\n",
    "                         out_coor_system=\"PROJCS['Africa_Albers_Equal_Area_Conic',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Albers'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',25.0],PARAMETER['Standard_Parallel_1',20.0],PARAMETER['Standard_Parallel_2',-23.0],PARAMETER['Latitude_Of_Origin',0.0],UNIT['Meter',1.0]]\",\n",
    "                         transform_method=\"\",\n",
    "                         in_coor_system=\"GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\",\n",
    "                         preserve_shape=\"NO_PRESERVE_SHAPE\",\n",
    "                         max_deviation=\"\",\n",
    "                         vertical=\"NO_VERTICAL\")\n",
    "\n",
    "## AfElSG Known + Possible Range\n",
    "arcpy.Project_management(in_dataset=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Spatial Data/SpatialData.gdb/AfElSg_Range_Dissolv\",\n",
    "                         out_dataset=\"afelsg_albers\",\n",
    "                         out_coor_system=\"PROJCS['Africa_Albers_Equal_Area_Conic',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Albers'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',25.0],PARAMETER['Standard_Parallel_1',20.0],PARAMETER['Standard_Parallel_2',-23.0],PARAMETER['Latitude_Of_Origin',0.0],UNIT['Meter',1.0]]\",\n",
    "                         transform_method=\"\",\n",
    "                         in_coor_system=\"GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\",\n",
    "                         preserve_shape=\"NO_PRESERVE_SHAPE\",\n",
    "                         max_deviation=\"\",\n",
    "                         vertical=\"NO_VERTICAL\")\n",
    "\n",
    "## WDPA\n",
    "arcpy.Clip_analysis(in_features=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Spatial Data/WDPA_August2013.gdb/WDPApoly_August2013\",\n",
    "                    clip_features=\"Continental Africa\",\n",
    "                    out_feature_class=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM.gdb/WDPA\",\n",
    "                    cluster_tolerance=\"\")\n",
    "\n",
    "arcpy.Project_management(in_dataset=\"WDPA\",\n",
    "                         out_dataset=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM.gdb/WDPA_Albers\",\n",
    "                         out_coor_system=\"PROJCS['Africa_Albers_Equal_Area_Conic',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Albers'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',25.0],PARAMETER['Standard_Parallel_1',20.0],PARAMETER['Standard_Parallel_2',-23.0],PARAMETER['Latitude_Of_Origin',0.0],UNIT['Meter',1.0]]\",\n",
    "                         transform_method=\"\",\n",
    "                         in_coor_system=\"GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\",\n",
    "                         preserve_shape=\"NO_PRESERVE_SHAPE\",\n",
    "                         max_deviation=\"\",\n",
    "                         vertical=\"NO_VERTICAL\")\n",
    "\n",
    "arcpy.Dissolve_management(in_features=\"WDPA_Albers\",\n",
    "                          out_feature_class=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM.gdb/WDPA_Albers_dissolv\",\n",
    "                          dissolve_field=\"\", statistics_fields=\"\", multi_part=\"MULTI_PART\", unsplit_lines=\"DISSOLVE_LINES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AFELSG Area outside WDPA\n",
    "arcpy.Erase_analysis(in_features=\"afelsg_albers\",\n",
    "                     erase_features=\"WDPA_Albers_dissolv\",\n",
    "                     out_feature_class=\"afelsg_albers_outside_wdpa\",\n",
    "                     cluster_tolerance=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSM_Dispersal: - Set Zeroes to Null\n",
    "arcpy.gp.SetNull_sa(\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/PanAfEl_HSM_Dispersal.tif\",\n",
    "                    \"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/PanAfEl_HSM_Dispersal.tif\",\n",
    "                    \"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM_Dispersal_SetNull.tif\",\n",
    "                    '\"Value\" = 0')\n",
    "\n",
    "# Convert HSM_Dispersal Raster to Polygon\n",
    "arcpy.RasterToPolygon_conversion(in_raster=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM_Dispersal_SetNull.tif\",\n",
    "                                 out_polygon_features=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM.gdb/HSM_Dispersal_Poly\",\n",
    "                                 simplify=\"SIMPLIFY\",\n",
    "                                 raster_field=\"Value\",\n",
    "                                 create_multipart_features=\"MULTIPLE_OUTER_PART\",\n",
    "                                 max_vertices_per_feature=\"\")\n",
    "\n",
    "# Project the Dispersal Range to Albers\n",
    "arcpy.Project_management(in_dataset=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM.gdb/HSM_Dispersal_Poly\", \n",
    "                         out_dataset=\"C:/Users/jake_/Dropbox/Research_PanAfEl/Analyses/HSM/HSM.gdb/HSM_Dispersal_Poly_Albers\", \n",
    "                         out_coor_system=\"PROJCS['Africa_Albers_Equal_Area_Conic',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Albers'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',25.0],PARAMETER['Standard_Parallel_1',20.0],PARAMETER['Standard_Parallel_2',-23.0],PARAMETER['Latitude_Of_Origin',0.0],UNIT['Meter',1.0]]\",\n",
    "                         transform_method=\"\", \n",
    "                         in_coor_system=\"GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\",\n",
    "                         preserve_shape=\"NO_PRESERVE_SHAPE\",\n",
    "                         max_deviation=\"\",\n",
    "                         vertical=\"NO_VERTICAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HSM outside of WDPA\n",
    "arcpy.Erase_analysis(in_features=\"HSM_Dispersal_Poly_Albers\",\n",
    "                     erase_features=\"WDPA_Albers_dissolv\",\n",
    "                     out_feature_class=\"hsm_dispersal_albers_outside_wdpa\",\n",
    "                     cluster_tolerance=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dispersal, core and AfElSG range areal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read these values in from ArcMAP\n",
    "hsm_dispersal_area = 18169219021690.476563  / 1000000\n",
    "afelsg_K_P_area = 3129540087056.136719  / 1000000\n",
    "africa_area = 29282962929907.191406  / 1000000\n",
    "afelsg_K_P_area_outside_wdpa = 1795945605785.407715  / 1000000\n",
    "hsm_dispersal_area_outside_wdpa = 15458775026175.25  / 1000000\n",
    "\n",
    "print('HSM Dispersal Area km2: {0}'.format(hsm_dispersal_area))\n",
    "print('AfElSG Area km2: {0}'.format(afelsg_K_P_area))\n",
    "print('Continental Africa Area km2: {0}'.format(africa_area))\n",
    "print('AfElSG Area Outside WDPA km2: {0}'.format(hsm_dispersal_area_outside_wdpa))\n",
    "print('HSM Dispersal Area Outside WDPA km2: {0}'.format(hsm_dispersal_area_outside_wdpa))\n",
    "print('AfElSG area fraction of HSM: {0}%'.format((afelsg_K_P_area / hsm_dispersal_area)*100))\n",
    "print('Fraction of HSM to African Continent Area: {0}%'.format((hsm_dispersal_area /africa_area)*100))\n",
    "print('Fraction of AfElSG to African Continent Area: {0}%'.format((afelsg_K_P_area / africa_area)*100))\n",
    "print('Fraction of HSM Outside of WDPA to overall HSM Area: {0}%'.format((hsm_dispersal_area_outside_wdpa / hsm_dispersal_area)*100))\n",
    "print('Fraction of AfElSG Outside of WDPA to overall AfElSG Area: {0}%'.format((afelsg_K_P_area_outside_wdpa / afelsg_K_P_area)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
